# -*- coding: utf-8 -*-
"""final_draft.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14bIehtT-s1MIkETQAwR6hb-j4d_5h7OH

## Quantitative Risk Management Project -- VaR Construction and Backtesting using different models and tests.
"""

!pip install yfinance==0.2.50

"""Data acquisition"""

import pandas as pd
import yfinance as yf
tickers = ['AMZN', 'DIS', 'AAPL', 'MSFT', 'JPM', 'F', 'PG',  '^GSPC',  '^IXIC'] #added s&p500, nasdaq
start_date = '2009-06-10'
end_date = '2024-12-12'

combined_data = pd.DataFrame()
for ticker in tickers:
    print(f"Downloading the data for  {ticker}...")

    data = yf.download(ticker, start=start_date, end=end_date, interval='1d')
    # Extract only column "Adj Close"
    if not data.empty:
        combined_data[ticker] = data['Adj Close']
    else:
        print(f"Warning: No data found for {ticker}.")

# Save combined data in  .CSV
combined_data.to_csv('combined_data_adj_close.csv')
print("Data succesfully  saved in csv")

import numpy as np
import matplotlib.pyplot as plt

file_path = 'combined_data_adj_close.csv'
prices = pd.read_csv(file_path, index_col='Date', parse_dates=True)
prices_out_of_sample = prices.loc['2022-01-01': '2024-12-20'] # test set = 3 years
print(prices_out_of_sample.head())
prices=prices.loc[:'2021-12-31']    # ONLY SET 12 years FOR MODEL ESTIMATION

# Extract only  Adj Close prices column for each ticker
tickers = prices.columns
print("\nTickers found:", tickers)

# create dictionary to store prices and ticker together
prices_dict = {ticker: prices[ticker] for ticker in tickers}

for ticker in tickers:
    print(f"\nPrice for {ticker}:")
    print(prices_dict[ticker].head())

# RETURNS for each ticker
#returns = prices.pct_change()
returns = (prices.values[1:] / prices.values[:-1]) - 1
#returns= (returns - returns.mean()) / returns.std()     #  standardization

#print(returns.head())
returns_df = pd.DataFrame(returns, columns=prices.columns, index=prices.index[1:])
print(returns_df.head())

# LOG RETURNS for each ticker
log_returns = np.log(prices / prices.shift(1))
print("\nLog returns (first rows):")
print(log_returns.head())

# Save returns in file CSV
#returns.to_csv('simple_returns.csv')
log_returns.to_csv('log_returns.csv')
print("\nReturns saved in 'simple_returns.csv' and 'log_returns.csv'.")

########## FIGURES-----------

# PRICES: Plot each ticker in a separate graph
for ticker in prices.columns:
    plt.figure(figsize=(6, 3))  # Create a new figure for each ticker
    plt.plot(prices.index, prices[ticker], label=f'{ticker} Prices')
    plt.title(f'{ticker} Prices')
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# RETURNS: Plot each ticker's returns
for ticker in returns_df.columns:
    plt.figure(figsize=(6, 3))  # Create a new figure for each ticker
    plt.plot(returns_df.index, returns_df[ticker], label=f'{ticker} Returns', color='orange')
    plt.title(f'{ticker} Returns')
    plt.xlabel('Date')
    plt.ylabel('Returns')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# for ticker in returns_df.columns:
#     plt.figure(figsize=(6, 3))  # Create a new figure for each ticker
#     plt.plot(log_returns.index, log_returns[ticker], label=f'{ticker} LogReturns', color='orange')
#     plt.title(f'{ticker} LogReturns')
#     plt.xlabel('Date')
#     plt.ylabel('LogReturns')
#     plt.legend()
#     plt.grid(True)
#     plt.tight_layout()
#     plt.show()

def calculate_statistics(returns):
    mean = np.mean(returns)
    median = np.median(returns)
    std_dev = np.std(returns)
    var_95 = np.percentile(returns, 5)  # 5th percentile for 95% VaR
    var_99 = np.percentile(returns, 1)  # 1st percentile for 99% VaR
    max_value = np.max(returns)
    min_value = np.min(returns)

    print("Statistics:")
    print(f"Mean: {mean:.6f}")
    print(f"Median: {median:.6f}")
    print(f"Standard Deviation: {std_dev:.6f}")
    print(f"95% VaR (5th percentile): {var_95:.6f}")
    print(f"99% VaR (1st percentile): {var_99:.6f}")
    print(f"Maximum Value: {max_value:.6f}")
    print(f"Minimum Value: {min_value:.6f}")

# Ex Calculate for one ticker
calculate_statistics(returns)

# Loop through all tickers in a DataFrame
for ticker in returns_df.columns:
    print(f"\nStatistics for {ticker}:")
    calculate_statistics(returns_df[ticker].dropna().values)

def plot_statistics(returns_df):
    # Calculate statistics for all tickers
    stats = {
        'Mean': [],
        'Median': [],
        'Std Dev': [],
        '99 percentile': []
    }

    tickers = returns_df.columns
    for ticker in tickers:
        returns = returns_df[ticker].dropna().values
        stats['Mean'].append(np.mean(returns))
        stats['Median'].append(np.median(returns))
        stats['Std Dev'].append(np.std(returns))
        #stats['95% percentile'].append(np.percentile(returns, 5))
        stats['99 percentile'].append(-np.percentile(returns, 1))

    # Bar Chart to Compare Mean, Std Dev, and VaR across tickers
    x = np.arange(len(tickers))  # Index for tickers
    width = 0.2

    plt.figure(figsize=(7, 4))

    plt.bar(x - width, stats['Mean'], width, label='Mean')
    plt.bar(x, stats['Std Dev'], width, label='Std Dev')
    plt.bar(x + width, stats['99 percentile'], width, label='99 percentile')

    plt.xticks(x, tickers, rotation=45)
    plt.xlabel('Tickers')
    plt.ylabel('Value')
    plt.title('Comparison of Statistics Across Tickers')
    plt.legend()
    plt.grid(axis='y')
    plt.tight_layout()
    plt.show()

    # Box Plot for Distribution of Returns
    plt.figure(figsize=(7, 4))
    returns_df.boxplot()
    plt.title('Boxplot of Returns for All Tickers')
    plt.ylabel('Returns')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

plot_statistics(returns_df)

!pip install arch

"""## Garch (2,2) Model fit in sample data"""

from arch import arch_model
from scipy.stats import t, norm

# Dictionary
garch_models = {}

for ticker in returns_df.columns:
    print(f"Adapting GARCH for  {ticker}...")
    returns = returns_df[ticker].dropna()
    model = arch_model(returns, mean='Zero', vol='Garch', p=2, q=2, dist='t')
    # model = arch_model(returns, mean='Zero', vol='EGarch', p=1, q=1, dist='t')
    # model = arch_model(returns, mean='Zero', vol='Garch', p=1, q=1, dist= 'normal')
    res = model.fit(update_freq=5, disp='off')
    garch_models[ticker] = res
    print(res.summary())


confidence_level = 0.01
z = norm.ppf(1 - confidence_level)

for ticker, res in garch_models.items():
    plt.figure(figsize=(7,3.5))
    # realized volatility as absolute value of returns
    realized_vol = returns_df[ticker].abs()
    plt.plot(realized_vol, label='Realized Volatility', color='blue', alpha=0.7)
     #conditional volatilty
    conditional_vol = res.conditional_volatility
    plt.plot(conditional_vol, label='Conditional Volatility', color='orange')

    plt.title(f"Realized vs Conditional Volatility for {ticker}")
    plt.xlabel("Date")
    plt.ylabel("Volatility")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

dof = res.params['nu']
quantile_t = t.ppf(1 - confidence_level, df=dof)
dof, quantile_t
quantile_5 = t.ppf(1 - 0.05, df=dof)

for ticker, res in garch_models.items():
    plt.figure(figsize=(7,3.5))

    # conditional
    conditional_vol = res.conditional_volatility
    # var computation
    #VaR = conditional_vol * z
    VaR_99 = conditional_vol * quantile_t
    VaR_95 = conditional_vol * quantile_5

    # Plot
    plt.plot(returns_df.index, returns_df[ticker], label='Returns', color='blue')
    plt.plot(returns_df.index, -VaR_99, label='VaR 99%', color='red', alpha=0.7)
    plt.plot(returns_df.index, -VaR_95, label='VaR 95%', color='green', alpha=0.7)

    plt.title(f"Daily Returns and VaR for {ticker}")
    plt.xlabel("Date")
    plt.ylabel("Returns / VaR")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""## Historical var"""

def empirical_quantile(returns, alpha=0.01):  # python dovrebbe farlo gia di default
    """
    computing the empirical quantile using formula from prof slides
    Returns:
    q_alpha: float, empirical quantile
    """
    # ordering returns (ascending)
    sorted_returns = np.sort(returns)
    #  kth position in ordered distribution
    T = len(sorted_returns)
    k = T * alpha
     # interpolating
    m = int(np.ceil(k))  # first integer value >= k
    m_minus_1 = m - 1    # previous value
    # Interpolation
    if m < T:
        q_alpha = (
            (m - k) * sorted_returns[m_minus_1] + (k - (m_minus_1)) * sorted_returns[m]
        )
    else:
        q_alpha = sorted_returns[-1]  # if m is bigger than T, use the last value

    return q_alpha


def historical_var(returns, alpha=0.01):
    return np.percentile(returns, alpha * 100)
historical_var(returns, 0.01)


def cornish_fisher_var(returns, alpha=0.01):
    """
    - VaR: corrected var value for skewness and  kurtosis, useful if we have few observations
    """
    mu = np.mean(returns)
    sigma = np.std(returns)
    skew = pd.Series(returns).skew()
    kurt = pd.Series(returns).kurt()

    # quantile from standard normal
    qz_alpha = norm.ppf(alpha)
    # cornish-Fisher adjustment
    q_cf = (
        qz_alpha +
        (skew / 6) * (qz_alpha**2 - 1) +
        (kurt / 24) * (qz_alpha**3 - 3 * qz_alpha) -
        (skew**2 / 36) * (2 * qz_alpha**3 - 5 * qz_alpha)
    )

    # VaR
    var = mu + sigma * q_cf
    return var


def historical_var_horizon(returns, alpha=0.01, horizon=1, use_square_root=False):
    """
    computing var, (optional : set time horizon)
    Parameters:
    - horizon: int, temporal horizon in days  (default 1)
    - use_square_root: bool, if True, use square root del tempo to scale  VaR
    """
    var = np.percentile(returns, alpha * 100)

    if use_square_root and horizon > 1:
        var *= np.sqrt(horizon)

    return var


for ticker in tickers:
    returns = returns_df[ticker].dropna()

    # Historical Simulation
    hs_var = historical_var_horizon(returns,  horizon=10, alpha=0.01,  use_square_root=False)
    # Cornish-Fisher Approximation
    cf_var = cornish_fisher_var(returns, alpha=0.01)
    print(f"{ticker}: Historical VaR (99%): {hs_var:.5f}, Cornish-Fisher VaR (99%): {cf_var:.5f}")


for ticker in tickers:
  returns = returns_df[ticker].dropna()
  var_empirical_app = empirical_quantile(returns, alpha=0.01)
  print(f"{ticker}: empirical distribution approximation for var 99% : {var_empirical_app:.5f}")


for ticker in returns_df.columns:
    print(f"\nCalculating annual historical VaR for {ticker}...")
    returns = returns_df[ticker]
    annual_groups = returns.groupby(returns.index.year)
    for year, annual_returns in annual_groups:
        var_99 = historical_var(annual_returns, 0.01)
        print(f"Historical VaR (99%) for {ticker} in {year}: {var_99:.5f}")

for ticker in returns_df.columns:
  returns = returns_df[ticker].dropna().values
  var_99_hist = np.percentile(returns, 1)  #  (VaR  99%)
  var_95_hist = np.percentile(returns, 5)   #  (VaR  95%)
  print(f"Historical VaR (99%): {var_99_hist} for {ticker}, Historical VaR (95%): {var_95_hist} for {ticker}")

"""## Extreme value theory - GEV with block maxima"""

from scipy.stats import genextreme, genpareto, expon, probplot

def calculate_block_maxima_all_tickers(returns_df, block_size):
    """
    compute block maxima using negative returns.
    block_maxima_dict: dict, block maxima for each ticker
    """
    block_maxima_dict = {}
    for ticker in returns_df.columns:
        returns = -returns_df[ticker].dropna().values  # Use negative returns for losses
        block_maxima = [
            max(returns[i:i + block_size]) for i in range(0, len(returns), block_size)
        ]
        block_maxima_dict[ticker] = np.array(block_maxima)
    return block_maxima_dict

# Calculate block maxima
block_size = 21
block_maxima_dict = calculate_block_maxima_all_tickers(returns_df, block_size)
alpha=0.01
def fit_gev(block_maxima):
    # Adapt Generalized Extreme Value (GEV)  to maxima
    params_gev = genextreme.fit(block_maxima)
    shape, loc, scale = params_gev
    print(f"GEV Parameters: Shape={shape:.4f}, Scale={scale:.4f}, Loc={loc:.4f}")

    plt.figure(figsize=(7, 3.5))
    x = np.linspace(min(block_maxima), max(block_maxima), 100)
    pdf = genextreme.pdf(x, shape, loc=loc, scale=scale)
    plt.hist(block_maxima, bins=30, density=True, alpha=0.6, color='blue', label='Block Maxima Histogram')
    plt.plot(x, pdf, 'r-', lw=2, label='GEV Fit')
    plt.title(f'Fitted GEV to Block Maxima for {ticker} ')
    plt.xlabel('Block Maxima')
    plt.ylabel('Density')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    return params_gev


gev_params_dict = {}

for ticker, block_maxima in block_maxima_dict.items():
    print(f"\n Finding GEV distribution for {ticker}...")
    gev_params_dict[ticker] = fit_gev(block_maxima)

for ticker, params in gev_params_dict.items():
    print(f"{ticker}: Shape={params[0]:.4f}, Loc={params[1]:.4f}, Scale={params[2]:.4f}")


confidence_level = 0.99
var_gev_dict = {}

def calculate_residuals(block_maxima, shape, loc, scale):
    residuals = (1 + shape * (block_maxima - loc) / scale) ** (-1 / shape)
    return residuals

def qq_plot_residuals(residuals, ticker):

    plt.figure(figsize=(5, 4.5))
    probplot(residuals, dist=expon, plot=plt)
    plt.title(f"QQ-Plot of Residuals for {ticker}")
    plt.xlabel("Theoretical Quantiles (Exponential)")
    plt.ylabel("Sample Quantiles")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# compute  and plot residuals for each ticker
for ticker, block_maxima in block_maxima_dict.items():
    shape, loc, scale = gev_params_dict[ticker]
    residuals = calculate_residuals(block_maxima, shape, loc, scale)
    qq_plot_residuals(residuals, ticker)


def calculate_var_gev(shape, loc, scale, block_size, alpha):
    m = block_size
    if shape != 0:
        var = loc - (scale / shape) * (1 - (-m * np.log(1 - alpha)) ** (-shape))
    else:
        var = loc - scale * np.log(-m * np.log(1 - alpha))
    return var

# Calculate and print VaR for each ticker
var_gev_dict = {}


for ticker, params in gev_params_dict.items():
    shape, loc, scale = params
    var_gev = calculate_var_gev(shape, loc, scale, block_size, confidence_level)
    var_gev_dict[ticker] = var_gev
    print(f"VaR (GEV) for {ticker} at {0.99 * 100:.1f}% confidence level: {var_gev:.4f}")

"""## Extreme value theory - Generalized Pareto Distribution"""

from scipy.stats import pareto, probplot, genpareto
from scipy.stats import linregress


def fit_generalized_pareto_with_percentile(returns):
    """
    fits a Generalized Pareto Distribution to exceedances above the 95th percentile threshold. (python function)
    Returns:
    params_gpd: tuple, fitted GPD parameters (shape, loc, scale)
    excesses: array, exceedances above the threshold
    """
    losses = -returns
    threshold = np.percentile(losses, 95)
    excesses = losses[losses > threshold] - threshold

    # fit of Generalized Pareto Distribution
    params_gpd = genpareto.fit(excesses)
    shape, loc, scale = params_gpd
    print(f"GPD Parameters: Shape={shape:.4f}, Scale={scale:.4f}, Loc={loc:.4f}")

    # Plot of  fit GPD
    plt.figure(figsize=(8, 4.5))
    x = np.linspace(0, max(excesses), 100)
    pdf = genpareto.pdf(x, shape, loc=loc, scale=scale)

    plt.hist(excesses, bins=50, density=True, alpha=0.6, color='blue', label='Excesses Histogram')
    plt.plot(x, pdf, 'r-', lw=2, label='GPD Fit')
    plt.title(f"Generalized Pareto Fit to Excesses for {ticker}")
    plt.xlabel("Excess")
    plt.ylabel("Density")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    return params_gpd, excesses


# Calculate Generalized Residuals
def calculate_residuals_gpd(excesses, params_pareto, threshold):

    shape, loc, scale = params_pareto
    residuals = (1 / shape) * np.log(1 + (shape * excesses) /(scale))
    return residuals

def qq_plot_residuals(residuals, ticker):

    plt.figure(figsize=(5, 4.5))
    probplot(residuals, dist=expon, plot=plt)
    plt.title(f"QQ-Plot of Residuals for {ticker}")
    plt.xlabel("Theoretical Quantiles (Exponential)")
    plt.ylabel("Sample Quantiles")
    plt.grid(True)
    plt.tight_layout()
    plt.show()


def var_pareto_gpd(returns, threshold, shape, scale, loc, confidence_level):

    T = len(returns)

    excesses = returns[returns > threshold]
    Tu = len(excesses)  # Number of exceedances

    print(f"Threshold (u): {threshold}")
    print(f"Shape (ξ): {shape}")
    print(f"Scale (σ): {scale}")
    print(f"Loc (μ): {loc}")
    print(f"Total Observations (T): {T}")
    print(f"Exceedances (T_u): {Tu}")

    alpha = 1 - confidence_level
    if Tu == 0:
        raise ValueError("No exceedances found. Adjust the threshold.")

    if shape != 0:
        var = threshold + (scale / shape) * (1 - ((T / Tu * alpha) ** (-shape)))

    else:  # limit case for  shape = 0
        var = threshold - scale - np.log(T / Tu * alpha)

    print(f"Calculated VaR: {var}")
    return var


confidence_level = 0.99
pareto_params_dict = {}
for ticker in tickers:
    print(f"Processing Pareto for {ticker}...")
    returns = returns_df[ticker].dropna().values

    #threshold = plot_mrl(returns, ticker)
    #print(f"Selected threshold for {ticker}: {threshold:.4f}")

    params_pareto, excesses = fit_generalized_pareto_with_percentile(returns)
    losses = -returns
    threshold = np.percentile(losses, 99)
    pareto_params_dict[ticker] = params_pareto

    shape, loc, scale = params_pareto
    T = len(returns)

    Tu = len(excesses)
    alpha= 1 - confidence_level
    var = var_pareto_gpd(returns, threshold, shape, scale, loc, confidence_level)
    print(f"Pareto VaR for {ticker} at {confidence_level * 100:.1f}% confidence level: {var:.4f}")

     # Calculate and plot residuals
    residuals = calculate_residuals_gpd(excesses, params_pareto, threshold)
    qq_plot_residuals(residuals, ticker)

"""## Filtered historical simulation first garch(1,1) and then egarch"""

n_simulations = 40  # Number of simulations
forecast_horizon = 250 # Time horizon
tickers = returns_df.columns
confidence_level = 0.99

for ticker in tickers:
    print(f"\nSimulating returns for {ticker}...")
    returns = returns_df[ticker].dropna()

    model = arch_model(returns, mean='Zero', vol='Garch', p=1, q=1)
    res = model.fit(disp='off')

    #  Extract standardized residuals and forecast volatility
    z = res.std_resid.dropna()
    forecasts = res.forecast(horizon=forecast_horizon, reindex=False)
    forecasted_variances = forecasts.variance.values[-1, :]  # Multi-step ahead variances
    forecasted_stds = np.sqrt(forecasted_variances)

    #  simulate future returns
    all_simulated_paths = np.zeros((forecast_horizon, n_simulations)) #matrix
    for sim in range(n_simulations):
        # Bootstrap from standardized residuals
        sampled_residuals = np.random.choice(z, size=forecast_horizon, replace=True)
        # scale residuals by forecasted volatility
        simulated_returns = sampled_residuals * forecasted_stds
        all_simulated_paths[:, sim] = simulated_returns #filling matrix with the  simulation (row)

    daily_VaRs = np.percentile(all_simulated_paths, (1 - confidence_level) * 100, axis=1)

    # Plot daily VaRs
    # plt.figure(figsize=(8, 4))
    # plt.plot(range(1, forecast_horizon + 1), daily_VaRs, marker='o', label='Daily VaR')
    # plt.title(f"Daily VaR Evolution for {ticker} ({forecast_horizon}-Day Horizon)")
    # plt.xlabel("Forecast Day")
    # plt.ylabel("Value-at-Risk")
    # plt.grid(True)
    # plt.legend()
    # plt.tight_layout()
    # plt.show()


    # Plot daily VaRs with realized returns
    plt.figure(figsize=(8, 4))
    realized_returns = returns_df[ticker].iloc[-forecast_horizon:].values
    plt.plot(range(1, forecast_horizon + 1), realized_returns, marker='o', linestyle='-', color='blue', alpha=0.6, label="Realized Returns")
    plt.plot(range(1, forecast_horizon + 1), simulated_returns, marker='o', linestyle='-', color='yellow', alpha=0.6, label="Simulated Mean Returns (FHS)")

    plt.plot(range(1, forecast_horizon + 1), daily_VaRs, marker='o', linestyle='--', color='red', label='Daily VaR (99%)')
    plt.title(f"Daily VaR vs Realized Returns for {ticker} ({forecast_horizon}-Day Horizon)")
    plt.xlabel("Forecast Day")
    plt.ylabel("Returns / VaR")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()


   # print(f"FHS VaR ({int(confidence_level * 100)}%) for {ticker}: {daily_VaRs:.5f}")

    #  Compute VaR for the last forecast horizon
    cumulative_returns = all_simulated_paths.sum(axis=0)
    VaR = np.percentile(cumulative_returns, (1 - confidence_level) * 100)

    print(f"FHS VaR ({int(confidence_level * 100)}%) for {ticker}: {VaR:.5f}")

    # Plot simulated trajectories
    plt.figure(figsize=(7, 3.5))
    for sim in range(min(100, n_simulations)):  # Plot only up to 100 paths for clarity
        plt.plot(all_simulated_paths[:, sim], alpha=0.6, lw=0.8)
    plt.title(f"Simulated FHS Returns for {ticker}  with GARCH ({forecast_horizon}-Day Horizon)")
    plt.xlabel("Time Steps")
    plt.ylabel("Returns")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # Plot histogram of cumulative returns and VaR
    plt.figure(figsize=(8, 4))
    plt.hist(cumulative_returns, bins=50, alpha=0.7, color='blue', label='Simulated Cumulative Returns ')
    plt.axvline(VaR, color='red', linestyle='--', label=f'VaR {int(confidence_level * 100)}%: {VaR:.5f}')
    plt.title(f"FHS Cumulative Returns and VaR for {ticker}")
    plt.xlabel("Cumulative Returns")
    plt.ylabel("Frequency")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()



###### ------------ FHS with EGARCH -------------------------#######
n_simulations = 40  # Number of simulations
forecast_horizon = 250  # Time horizon
tickers = returns_df.columns
confidence_level = 0.99

for ticker in tickers:
    print(f"\nSimulating returns for {ticker} with EGARCH...")
    returns = returns_df[ticker].dropna()

    # Initialize EGARCH model
    model = arch_model(returns, mean='Zero', vol='EGarch', p=1, q=1)
    res = model.fit(disp='off')

    #  Extract standardized residuals and initial volatility
    z = res.std_resid.dropna()  # Standardized residuals
    last_volatility = res.conditional_volatility.iloc[-1]

    # Simulate future returns and volatilities iteratively
    all_simulated_paths = np.zeros((forecast_horizon, n_simulations))
    for sim in range(n_simulations):
        simulated_path = []
        volatility_t = last_volatility

        for t in range(forecast_horizon):
            # Bootstrap a standardized residual
            z_t = np.random.choice(z)
            # Update volatility using EGARCH dynamics
            variance_t = (
                res.params['omega']
                + res.params['beta[1]'] * np.log(volatility_t**2)
                + res.params['alpha[1]'] * z_t
            )
            volatility_t = np.exp(variance_t / 2)
            # Simulate return
            simulated_return = z_t * volatility_t
            simulated_path.append(simulated_return)

        all_simulated_paths[:, sim] = simulated_path

    daily_VaRs = np.percentile(all_simulated_paths, (1 - confidence_level) * 100, axis=1)

    # # Plot daily VaRs
    # plt.figure(figsize=(8, 4))
    # plt.plot(range(1, forecast_horizon + 1), daily_VaRs, marker='o', label='Daily VaR')
    # plt.title(f"Daily VaR Evolution for {ticker} ({forecast_horizon}-Day Horizon)")
    # plt.xlabel("Forecast Day")
    # plt.ylabel("Value-at-Risk")
    # plt.grid(True)
    # plt.legend()
    # plt.tight_layout()
    # plt.show()

    # Plot daily VaRs with realized returns
    plt.figure(figsize=(8, 4))
    realized_returns = returns_df[ticker].iloc[-forecast_horizon:].values
    plt.plot(range(1, forecast_horizon + 1), realized_returns, marker='o', linestyle='-', color='blue', alpha=0.6, label="Realized Returns")
    #plt.plot(range(1, forecast_horizon + 1), simulated_mean_returns, marker='o', linestyle='-', color='yellow', alpha=0.6, label="Simulated Mean Returns (FHS)")
    plt.plot(range(1, forecast_horizon + 1), daily_VaRs, marker='o', linestyle='--', color='red', label='Daily VaR (99%)')
    plt.title(f"Daily VaR vs Realized Returns for {ticker} ({forecast_horizon}-Day Horizon)")
    plt.xlabel("Forecast Day")
    plt.ylabel("Returns / VaR")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()


    print(f"FHS VaR ({int(confidence_level * 100)}%) for {ticker}: {VaR:.5f}")

    #  Compute VaR for the last forecast horizon
    cumulative_returns = all_simulated_paths.sum(axis=0)
    VaR = np.percentile(cumulative_returns, (1 - confidence_level) * 100)

    print(f"FHS VaR ({int(confidence_level * 100)}%) for {ticker}: {VaR:.5f}")

    # Plot simulated trajectories
    plt.figure(figsize=(7, 3.5))
    for sim in range(min(100, n_simulations)):  # Plot only up to 100 paths for clarity
        plt.plot(all_simulated_paths[:, sim], alpha=0.6, lw=0.8)
    plt.title(f"Simulated FHS Returns for {ticker} with EGARCH ({forecast_horizon}-Day Horizon)")
    plt.xlabel("Time Steps")
    plt.ylabel("Returns")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # Plot histogram of cumulative returns and VaR
    plt.figure(figsize=(7, 3.5))
    plt.hist(cumulative_returns, bins=50, alpha=0.7, color='blue', label='Simulated Cumulative Returns')
    plt.axvline(VaR, color='red', linestyle='--', label=f'VaR {int(confidence_level * 100)}%: {VaR:.5f}')
    plt.title(f"FHS Cumulative Returns and VaR for {ticker} with EGARCH")
    plt.xlabel("Cumulative Returns")
    plt.ylabel("Frequency")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""## Dynamic quantile regression"""

import statsmodels.api as sm
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt

returns = (prices.values[1:] / prices.values[:-1]) - 1
returns_df = pd.DataFrame(returns, columns=prices.columns, index=prices.index[1:])

# Functions to add lags to data ----------
def add_lags(data, lags=2):
    lagged_data = data.copy()
    for lag in range(1, lags + 1):
        lagged_data[f'lag_{lag}'] = lagged_data['returns'].shift(lag)
    return lagged_data.dropna()

def add_lags_and_squared(data, lags=1):
    lagged_data = data.copy()
    for lag in range(1, lags + 1):
        lagged_data[f'lag_{lag}'] = lagged_data['returns'].shift(lag)
        lagged_data[f'lag_{lag}_squared'] = lagged_data[f'lag_{lag}'] ** 2
    return lagged_data.dropna()


def add_squared(data, lags=1):
    lagged_data = data.copy()
    for lag in range(1, lags + 1):

        lagged_data[f'lag_{lag}_squared'] = lagged_data['returns'].shift(lag) ** 2

    return lagged_data.dropna()  # Drop NaN values caused by shifting


alpha = 0.01  # confidence level of 99% (quantile 1%)
lags = 2

for ticker in returns_df.columns:
    print(f"\nQuantile Regression for {ticker}...")

    returns_ticker = returns_df[[ticker]].rename(columns={ticker: 'returns'})
    lagged_data = add_lags(returns_ticker, lags=lags)

    # prepare dependent and independent variables
    y = lagged_data['returns']
    X = lagged_data.drop(columns=['returns'])
    X = sm.add_constant(X)  # add intercept

    # Quantile Regression
    model = sm.QuantReg(y, X)
    quantile_model = model.fit(q=alpha)  # estimate
    print(quantile_model.summary())

    # var computation
    VaR_dynamic = quantile_model.predict(X)
    VaR_dynamic = pd.Series(VaR_dynamic, index=lagged_data.index, name='VaR_dynamic')

    #
    plt.figure(figsize=(8, 4.5))
    plt.plot(lagged_data.index, y, label="Returns", color="blue", alpha=0.6)
    plt.plot(VaR_dynamic.index, VaR_dynamic, label=f"Dynamic VaR ({int(alpha*100)}%)", color="red", linestyle="--")
    plt.fill_between(VaR_dynamic.index, VaR_dynamic, -0.5, color="red", alpha=0.2, label="Exceedance Zone")
    plt.title(f"Dynamic Quantile Regression VaR for {ticker}")
    plt.xlabel("Date")
    plt.ylabel("Returns")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()


    #Estimate Quantile Regressions at Multiple Quantiles (not just at the 1% level) to see what changes

    # Fit QR for different quantiles
    quantiles = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.75, 0.8, 0.9, 0.95]  # choose
    models = []

    example_data = returns_df[[ticker]].rename(columns={ticker: 'returns'})
    example_data['lag1'] = example_data['returns'].shift(1)
    example_data['lag1_squared'] = example_data['lag1'] ** 2
    example_data = example_data.dropna()

    # regression for every quantile
    for tau in quantiles:
        model = smf.quantreg('returns ~ lag1 + lag1_squared', example_data).fit(q=tau)
        models.append(model)

    # extract  coefficients
    for i, tau in enumerate(quantiles):
        print(f"Quantile {tau}:")
        print(models[i].params)

    # plot evolution of  coefficients
    taus = np.linspace(0.05, 0.95, 19)
    coefficients = []

    for tau in quantiles:
        model = smf.quantreg('returns ~ lag1 + lag1_squared', example_data).fit(q=tau)
        coefficients.append(model.params)

    coefficients = pd.DataFrame(coefficients, index=quantiles)
    plt.plot(quantiles, coefficients['lag1'], label='Lag 1 Coefficient')
    plt.fill_between(quantiles,
                    coefficients['lag1'] - 1.96 * model.bse['lag1'],
                    coefficients['lag1'] + 1.96 * model.bse['lag1'],
                    alpha=0.2)
    plt.xlabel('Quantile')
    plt.ylabel('Coefficient')

    plt.figure(figsize=(8, 5))


lags = 1  # Number of lags to include
# ----this time  add lags and lags squared ----
for ticker in returns_df.columns:
    print(f"\nQuantile Regression with lag and squared lag for {ticker}...")

    returns_ticker = returns_df[[ticker]].rename(columns={ticker: 'returns'})
    lagged_data = add_lags_and_squared(returns_ticker, lags=lags)

    # keep copy of original index
    lagged_data = lagged_data.dropna()
    y = lagged_data['returns']
    X = lagged_data.drop(columns=['returns'])
    X = sm.add_constant(X)  # add  intercept

    # Quantile Regression
    model = sm.QuantReg(y, X)
    quantile_model = model.fit(q=alpha)  # estimate
    print(quantile_model.summary())

    #  VaR dynamic
    VaR_dynamic = quantile_model.predict(X)
    VaR_dynamic = pd.Series(VaR_dynamic, index=lagged_data.index, name='VaR_dynamic')

    #
    plt.figure(figsize=(8, 4.5))
    plt.plot(lagged_data.index, y, label="Returns", color="blue", alpha=0.6)
    plt.plot(VaR_dynamic.index, VaR_dynamic, label=f"Dynamic VaR ({int(alpha*100)}%)", color="red", linestyle="--")
    plt.fill_between(VaR_dynamic.index, VaR_dynamic, -1, color="red", alpha=0.2, label="Exceedance Zone")
    plt.title(f"Dynamic Quantile Regression VaR for {ticker}")
    plt.xlabel("Date")
    plt.ylabel("Returns")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()


    #quantiles = np.linspace(0.01, 0.99, 12)  # Quantili da analizzare
    coefficients = []
    errors = []  # Per memorizzare gli errori standard

    for q in quantiles:
        quantile_model = model.fit(q=q)
        coefficients.append(quantile_model.params)
        errors.append(quantile_model.bse)

    coefficients_df = pd.DataFrame(coefficients, index=quantiles)
    errors_df = pd.DataFrame(errors, index=quantiles)  # Errori standard

    # effects of quantiles on coefficients
    plt.figure(figsize=(8, 4))
    plt.plot(coefficients_df.index, coefficients_df['lag_1_squared'], label='Lag 1 Squared Coefficient', color='orange')

    plt.xlabel('Quantile')
    plt.ylabel('Coefficient Value')
    plt.title(f'Coefficient Evolution Across Quantiles for {ticker}')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()


for ticker in returns_df.columns:
    print(f"\nQuantile Regression Coefficients for {ticker}...")

    returns_ticker = returns_df[[ticker]].rename(columns={ticker: 'returns'})
    data = add_lags_and_squared(returns_ticker, lags=lags)
    data = data.dropna()

    y = data['returns']
    X = data.drop(columns=['returns'])
    X = sm.add_constant(X)

    # list to save lag_1 coefficients
    lag_1_coefficients = []

    # quantile regression for each quantile
    for q in quantiles:
        model = sm.QuantReg(y, X)
        result = model.fit(q=q)
        lag_1_coefficients.append(result.params['lag_1'])  # Salva il coefficiente di lag_1

    # Plot  coefficients vs quantiles
    plt.figure(figsize=(6, 3))
    plt.plot(quantiles, lag_1_coefficients, marker='o', linestyle='-', color='blue')
    plt.title(f'Lag 1 Coefficient Evolution Across Quantiles for {ticker}')
    plt.xlabel('Quantile')
    plt.ylabel('Lag 1 Coefficient')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""## EVT and GARCH (with gev)"""

from scipy.stats import genextreme

block_size = 50
confidence_level = 0.99
gev_params_residuals = {}
var_gev_residuals = {}

for ticker, res in garch_models.items():
    print(f"\nProcessing GEV for standardized residuals of {ticker}...")

    # Extract standardized residuals from GARCH model
    standardized_residuals = res.std_resid

    block_maxima = [max(standardized_residuals[i:i + block_size])
                    for i in range(0, len(standardized_residuals), block_size)]
    block_maxima = np.array(block_maxima)

    #fit GEV to block maxima
    params = genextreme.fit(block_maxima)  # Fit the GEV distribution
    shape, loc, scale = params
    gev_params_residuals[ticker] = params
    print(f"GEV Parameters for {ticker}: Shape={shape:.4f}, Loc={loc:.4f}, Scale={scale:.4f}")

    # calculate GEV-based VaR for standardized residuals dynamically
    conditional_volatility = res.conditional_volatility
    quantile = loc - (scale / shape) * (1 - (-block_size * np.log(1 - alpha)) ** (-shape))
    daily_gev_var = conditional_volatility * quantile

    #daily_gev_var = conditional_volatility * genextreme.ppf(confidence_level, shape, loc=loc, scale=scale)

    # align with time index
    daily_gev_var_series = pd.Series(daily_gev_var, index=conditional_volatility.index)

    # plot returns and GEV-based VaR
    plt.figure(figsize=(8, 4.5))
    plt.plot(returns_df.index, returns_df[ticker], label='Returns', color='blue', alpha=0.6)
    plt.plot(daily_gev_var_series.index, -daily_gev_var_series, label='Daily GEV VaR (Dynamic)', color='red', alpha=0.7)

    plt.title(f"Daily Returns and Dynamic GEV VaR for {ticker}")
    plt.xlabel("Date")
    plt.ylabel("Returns / VaR")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    plt.hist(block_maxima, bins=30, density=True, alpha=0.6, color='blue', label='Block Maxima')
    x = np.linspace(min(block_maxima), max(block_maxima), 100)
    pdf = genextreme.pdf(x, shape, loc=loc, scale=scale)
    plt.plot(x, pdf, 'r-', lw=2, label='GEV Fit')
    plt.legend()
    plt.title("GEV Fit to Block Maxima")
    plt.show()

"""## Backtesting"""

from arch import arch_model
from tqdm import tqdm
from typing import Union, List, Dict
from scipy.stats import chi2
from scipy import stats, optimize
import time

alpha=0.01

# function from vartests library

# Definisce la funzione objective()
#che calcola la log-likelihood per vedere quanto i rendimenti standardizzati assomigliano a una normale standard N
#Confronta la log-likelihood ottimale con quella dell’ipotesi nulla N

def berkowitz_tail_test(
    pnl: pd.DataFrame,
    volatility_window: int = 252,
    var_conf_level: float = 0.99,
    conf_level: float = 0.99,
    random_seed: int = 443,
) -> dict:
    """ verify if conditional distributions of returns
        used in the VaR model is adherent to the data.
            pnl (dataframe):         pnl (distribution of profit and loss) or return
            volatility_window (int): window to calibrate volatility GARCH model
            random_seed (int):       integer value to set seed to random values of the optimizer
        Returns:
            answer (dict):           statistics and decision of the test
    """
    if not isinstance(pnl, pd.DataFrame):
        raise ValueError("Input must be dataframe.")

    print("Normalizing returns...")
    conditional_vol, conditional_mean = pd.DataFrame(), pd.DataFrame()
    for t in tqdm(range(pnl.shape[0] - volatility_window)):
        am = arch_model(
            pnl[t: volatility_window + t],
            vol="GARCH",
            dist="normal",
            rescale=False,
        ).fit(disp="off")
        cond_vol = am.forecast(horizon=1, reindex=False).variance.apply(np.sqrt)
        cond_mean = am.forecast(horizon=1, reindex=False).mean
        conditional_vol = pd.concat([conditional_vol, cond_vol])
        conditional_mean = pd.concat([conditional_mean, cond_mean])

    # standardized returns with unconditional mean and forecasted conditional volatility
    ret_padr = (pnl[volatility_window:].values.flatten() - pnl[volatility_window:].mean().values[0]) / conditional_vol.values.flatten()

    zeta = stats.norm.ppf(stats.norm.cdf(ret_padr))
    zeta[np.isinf(zeta)] = 0

    alpha = 1 - var_conf_level
    significance = 1 - conf_level

    # def objective(x):
    #     p1 = zeta[zeta < stats.norm.ppf(alpha)]
    #     p2 = zeta[zeta >= stats.norm.ppf(alpha)] * 0 + stats.norm.ppf(alpha)
    #     return -(
    #         np.sum(np.log(stats.norm.pdf((p1 - x[0]) / x[1] + epsilon) / x[1] + epsilon)) +
    #         np.sum(np.log(1 - stats.norm.cdf((p2 - x[0]) / x[1] + epsilon)))
    #     )

    def objective(x):
        epsilon = 1e-10  # Small value to prevent log(0)
        p1 = zeta[zeta < stats.norm.ppf(alpha)]
        p2 = zeta[zeta >= stats.norm.ppf(alpha)] * 0 + stats.norm.ppf(alpha)

        term1 = np.maximum(stats.norm.pdf((p1 - x[0]) / x[1]) / x[1], epsilon)
        term2 = np.maximum(1 - stats.norm.cdf((p2 - x[0]) / x[1]), epsilon)

        return -(
            np.sum(np.log(term1)) +
            np.sum(np.log(term2))
    )

    print("Optimizing...")
    start = time.time()

    np.random.seed(random_seed)
    bounds = [(-10, 10), (0.01, 3)]
    initial_guess = np.random.uniform(low=[-10, 0.01], high=[10, 3])

    result = optimize.minimize(
        fun=objective,
        x0=initial_guess,
        bounds=bounds,
        method='L-BFGS-B',
        options={'maxiter': 100, 'disp': False}
    )

    print(f"Elapsed time: {time.time() - start} s")

    best_solution = result.x
    uLL = -result.fun
    rLL = -objective([0, 1])
    LR = 2 * (uLL - rLL)
    chid = 1 - stats.chi2.cdf(LR, 2)

    decision = "Reject H0" if chid < significance else "Fail to Reject H0"
    H0 = "Distribution is Normal(0,1)"

    answer = {
        "solution": best_solution,
        "ull": uLL,
        "rll": rLL,
        "LR": LR,
        "chi square test": chid,
        "null hypothesis": H0,
        "decision": decision,
    }

    return answer


def tuff_test(violations, alpha):
    """
    TUFF Test Implementation: Time Until First Failure.
    - violations (array-like): Binary sequence of violations (1 = violation, 0 = no violation).
    - alpha (float): Confidence level for VaR (e.g., 0.01 for 99%).
    """
    pVaR = alpha  # Failure probability  # woul be  1- varlevel in  matlab
    if 1 not in violations.values:
        n = len(violations) + 1
        LRatioTUFF = -2 * (np.log(pVaR) + (n - 1) * np.log(1 - pVaR) + n * np.log(n) - (n - 1) * np.log(n - 1))
        p_value = 1 - chi2.cdf(LRatioTUFF, df=1)
        return {
            "time_to_failure": n,  # include 'time_to_failure' for consistency
            "likelihood_ratio": LRatioTUFF,
            "p_value": p_value,
            "decision": "Reject"
        }
    # time to first failure
    n = np.argmax(violations.values) + 1
    if n == 1:
        LRatioTUFF = -2 * np.log(pVaR)
    else:
        LRatioTUFF = -2 * (np.log(pVaR) + (n - 1) * np.log(1 - pVaR) + n * np.log(n) - (n - 1) * np.log(n - 1))

    # p-value based on chi-squared distribution
    p_value = 1 - chi2.cdf(LRatioTUFF, df=1)

    # decision
    decision = "Reject" if p_value < 0.05 else "Accept"

    return {
        "time_to_failure": n,
        "likelihood_ratio": LRatioTUFF,
        "p_value": p_value,
        "decision": decision
    }


def engle_manganelli_test(returns, var, alpha, lags=1):
    """
    - lags: Number of lags to include in the test.
    - dict: Contains test statistic and p-value.
    """
    # compute the vector Hit_t - alpha
    hits = ((returns < -var).astype(int) - alpha)

    # matrix of data with lags
    data = pd.DataFrame({"Hit": hits, "VaR": var})
    for i in range(1, lags + 1):
        data[f"Hit_lag_{i}"] = data["Hit"].shift(i)
        #data[f"VaR_lag_{i}"] = data["VaR"].shift(i)

    data = data.dropna()

    # matrix  Z and vector y
    Z = data[[f"Hit_lag_{i}" for i in range(1, lags + 1)] ].values
             #+ [f"VaR_lag_{i}" for i in range(1, lags + 1)]].values
    y = data["Hit"].values


    print(f"Matrix Z for ticker:")
    print(pd.DataFrame(Z, columns=[f"Hit_lag_{i}" for i in range(1, lags + 1)] ))
                                     #+ [f"VaR_lag_{i}" for i in range(1, lags + 1)]))
    print(f"Rank of Z: {np.linalg.matrix_rank(Z)}, Number of columns : {Z.shape[1]}")

    if np.linalg.matrix_rank(Z) < Z.shape[1]:   # control to see if the matrix is singular then there is multicollinearity
        print("Warning: Matrix Z is singular or poorly conditioned.")
        return {"test_statistic": np.nan, "p_value": np.nan}

    # Standardization of   Z
    Z = (Z - Z.mean(axis=0)) / Z.std(axis=0)

    # estimate of coefficients
    try:
        beta_hat = np.linalg.inv(Z.T @ Z) @ (Z.T @ y)
    except np.linalg.LinAlgError:
        print("Error: Singular matrix encountered during inversion.")
        return {"test_statistic": np.nan, "p_value": np.nan}

    residuals = y - Z @ beta_hat
    sigma2 = residuals.var()

    # test statistic
    DQ_statistic = beta_hat.T @ Z.T @ Z @ beta_hat / (alpha * (1 - alpha))
    p_value = 1 - chi2.cdf(DQ_statistic, df=Z.shape[1])

    return {"test_statistic": DQ_statistic, "p_value": p_value}


def christoffersen_test(tot_returns_out, violations, confidence_level=0.99):
    n = len(tot_returns_out)
    confidence_level = 0.99
    # Check for insufficient data
    if n == 0 or np.sum(violations) == 0 or np.sum(violations) == n:
        return {
            "Unconditional Coverage": {"LR Statistic": np.nan, "p-value": np.nan, "Decision": "Insufficient Violations"},
            "Independence": {"LR Statistic": np.nan, "p-value": np.nan, "Decision": "Insufficient Violations"},
            "Conditional Coverage": {"LR Statistic": np.nan, "p-value": np.nan, "Decision": "Insufficient Violations"}}
    # Total number of violations
    num_violations = np.sum(violations)
    p_var = 1 - confidence_level
    epsilon = 1e-8

    # Unconditional Coverage Test (Kupiec)
    pi_hat = num_violations / n
    #L_unconditional = (p_var ** num_violations) * ((1 - p_var) ** (n - num_violations))    # this formulation did not work
    #L_restricted = (pi_hat ** num_violations) * ((1 - pi_hat) ** (n - num_violations))
    #lr_uc = -2 * (np.log(L_unconditional) + 2*np.log(L_restricted))
    # Debug Print intermediate results
    lr_uc = -2 *(num_violations * np.log(p_var ) + (n- num_violations)* np.log(1 - p_var) ) +2*( (n - num_violations)* np.log((1 - pi_hat))
    + num_violations * np.log(pi_hat ))
    print(f"Unconditional Coverage:  lr_uc={lr_uc}")
    p_value_uc = 1 - chi2.cdf(lr_uc, df=1)

    # Transition Counts for Independence Test
    n00, n01, n10, n11 = 0, 0, 0, 0

    # Iterate over the violations vector
    for i in range(1, len(violations)):
        if violations.iloc[i - 1] == 0 and violations.iloc[i] == 0:
            n00 += 1
        elif violations.iloc[i - 1] == 0 and violations.iloc[i] == 1:
            n01 += 1
        elif violations.iloc[i - 1] == 1 and violations.iloc[i] == 0:
            n10 += 1
        elif violations.iloc[i - 1] == 1 and violations.iloc[i] == 1:
            n11 += 1

    # Add epsilon for numerical stability
    n00 += epsilon
    n01 += epsilon
    n10 += epsilon
    n11 += epsilon
    #  Print transition counts
    print(f"Transitions: n00={n00}, n01={n01}, n10={n10}, n11={n11}")
    # Probabilities of Transitions
    p_01 = (n01 +epsilon) / (n00 + n01 + epsilon)
    p_11 = (n11 +epsilon)/ (n10 + n11 + epsilon)
    p = num_violations / n

    # Likelihoods for Independence Test
    L_m = ((1 - p_01 +epsilon ) ** n00) * ((p_01+epsilon ) ** n01) * ((1 - p_11+epsilon ) ** n10) * ((p_11 +epsilon ) ** n11)
    L_b =( (1 - p +epsilon  ) ** (n00 + n10) ) * ((p +epsilon ) ** (n01 + n11))

    print(f"Likelihoods: L_m={L_m}, L_b={L_b}")
    # Log-Likelihood Ratio for Independence Test
    lr_ind =  (2*np.log(L_m) - 2* np.log(L_b)) if L_m > 0 and L_b > 0 else np.nan
    # Conditional Coverage Test
    lr_cc = lr_uc + (lr_ind if not np.isnan(lr_ind) else 0)

    # P-Values
    p_value_uc = 1 - chi2.cdf(lr_uc, df=1)
    p_value_ind = 1 - chi2.cdf(lr_ind, df=1) if not np.isnan(lr_ind) else np.nan
    p_value_cc = 1 - chi2.cdf(lr_cc, df=2) if not np.isnan(lr_ind) else np.nan

    return {
        "Unconditional Coverage": {
            "LR Statistic": lr_uc,
            "p-value": p_value_uc,
            "Decision": "Reject H0" if p_value_uc < 0.05 else "Fail to Reject H0"
        },
        "Independence": {
            "LR Statistic": lr_ind,
            "p-value": p_value_ind,
            "Decision": "Reject H0" if p_value_ind < 0.05 else "Fail to Reject H0"
        },
        "Conditional Coverage": {
            "LR Statistic": lr_cc,
            "p-value": p_value_cc,
            "Decision": "Reject H0" if p_value_cc < 0.05 else "Fail to Reject H0"
        }
    }


def kupiec_test(violations_series, confidence_level=0.99):
    """
    Parameters:
    - violations_series (pd.Series): Series containing 0 (no violation) and 1 (violation).
    - confidence_level (float): Expected confidence level of the VaR model (default: 0.99).

    Returns:
    - dict: A dictionary containing the test statistic, p-value, and test decision.
    """

    num_violations = violations_series.sum()
    # Total number of observations
    n = len(violations_series)

    # Check for insufficient data
    if n == 0 or num_violations == 0 or num_violations == n:
        return {
            "LR Statistic": np.nan,
            "p-value": np.nan,
            "Decision": "Insufficient Violations"
        }

    # Expected violation probability
    p_var = 1 - confidence_level
    # Estimated violation probability from the sample
    pi_hat = num_violations / n

    # Log-likelihood calculations
    L_unrestricted = (pi_hat ** num_violations) * ((1 - pi_hat) ** (n - num_violations))
    L_restricted = (p_var ** num_violations) * ((1 - p_var) ** (n - num_violations))

    # Kupiec Likelihood Ratio test statistic
    lr_stat = -2 * (np.log(L_restricted) - np.log(L_unrestricted))

    # Compute p-value for the test (chi-squared distribution with 1 degree of freedom)
    p_value = 1 - chi2.cdf(lr_stat, df=1)

    # Decision based on a 5% significance level
    decision = "Reject H0" if p_value < 0.05 else "Fail to Reject H0"

    return {
        "LR Statistic": lr_stat,
        "p-value": p_value,
        "Decision": decision
    }

def failure_rate_fun(violations: Union[List[int], pd.Series, pd.DataFrame]) -> Dict:
    if isinstance(violations, pd.core.series.Series) or isinstance(
        violations, pd.core.frame.DataFrame
    ):
        N = violations.sum()
    elif isinstance(violations, List) or isinstance(violations, np.ndarray):
        N = sum(violations)
    else:
        raise ValueError("Input must be list, array, series or dataframe.")
    TN = len(violations)

    answer = {"failure rate": N / TN}
    print(f"Failure rate of {round((N/TN)*100,2)}%")
    return answer

"""## backtesting historical"""

# Dictionary
garch_models = {}

for ticker in returns_df.columns:
    # print(f"Adattamento del modello GARCH per {ticker}...")
    returns = returns_df[ticker].dropna().iloc[:-300]
    # model = arch_model(returns, mean='Zero', vol='Garch', p=2, q=2, dist='t')
    model = arch_model(returns, mean='Zero', vol='EGarch', p=1, q=1, dist='t', rescale=False)
    res = model.fit(update_freq=5, disp='off')
    garch_models[ticker] = res
    # print(res.summary())

from scipy.stats import genextreme

block_size = 50
confidence_level = 0.99
gev_params_residuals = {}
var_gev_residuals = {}

for ticker, res in garch_models.items():
    print(f"\nProcessing GEV for standardized residuals of {ticker}...")

    # Extract standardized residuals from GARCH model
    standardized_residuals = res.std_resid
    print(len(standardized_residuals))
    num_blocks = len(standardized_residuals) // block_size
    block_maxima = [max(standardized_residuals[i * block_size:(i + 1) * block_size])
                    for i in range(num_blocks)]
    block_maxima = np.array(block_maxima)

    # Validate block maxima length
    if len(block_maxima) < 10:
        print(f"Warning: Too few block maxima ({len(block_maxima)}) for reliable GEV fitting.")
        continue

    # Fit GEV to block maxima
    try:
        params = genextreme.fit(block_maxima)
    except RuntimeError as e:
        print(f"GEV fitting failed for {ticker}: {e}")
        continue

    shape, loc, scale = params
    gev_params_residuals[ticker] = params
    print(f"GEV Parameters for {ticker}: Shape={shape:.4f}, Loc={loc:.4f}, Scale={scale:.4f}")

    # Calculate GEV-based VaR for standardized residuals dynamically
    conditional_volatility = res.conditional_volatility
    if abs(shape) < 1e-4:
        print("Warning: Shape parameter is close to zero. Using alternative formula.")
        # quantile = loc - scale * np.log(-block_size * np.log(1 - confidence_level))
        quantile = loc - scale * np.log(-block_size * np.log(1 - alpha))
    else:
        quantile = loc - (scale / shape) * (1 - (-block_size * np.log(1 - alpha)) ** (-shape))

    print(quantile)
    daily_gev_var = conditional_volatility * quantile


    # Align with time index
    daily_gev_var_series = pd.Series(daily_gev_var, index=conditional_volatility.index).reindex(returns_df[ticker].iloc[:-300].index, method='ffill')
    # print(len(daily_gev_var))
    # Plot results
    # plt.figure(figsize=(8, 4.5))
    # plt.plot(returns_df.iloc[:-300].index, returns_df[ticker].iloc[:-300], label='Returns', color='blue', alpha=0.6)
    # plt.plot(daily_gev_var_series.index, -daily_gev_var_series, label='Daily GEV VaR (Dynamic)', color='red', alpha=0.7)
    # plt.title(f"Daily Returns and Dynamic GEV VaR for {ticker}")
    # plt.xlabel("Date")
    # plt.ylabel("Returns / VaR")
    # plt.legend()
    # plt.grid(True)
    # plt.tight_layout()
    # plt.show()

"""## Backtesting historical VaR"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

# Parameters
rolling_window_size = 252  # Length of the rolling window
confidence_level = 0.99
alpha = 1 - confidence_level

# Data out-of-sample
out_of_sample_returns = (prices_out_of_sample.values[1:] / prices_out_of_sample.values[:-1]) - 1
out_of_sample_returns_df = pd.DataFrame(
    out_of_sample_returns, columns=prices_out_of_sample.columns, index=prices_out_of_sample.index[1:]
)
out_of_sample_returns_df= out_of_sample_returns_df.iloc[:300]


def rolling_historical_var_forecast(returns_df, out_of_sample_returns_df, rolling_window=252, confidence_level=0.99):
    historical_var_predictions = {}
    backtest_results = {}

    for ticker in out_of_sample_returns_df.columns:
        print(f"\nRunning Rolling Window Historical VaR for {ticker}...")

        returns_in_sample = returns_df[ticker].dropna().iloc[:-300]  # Use only historical in-sample data
        returns_out_of_sample = out_of_sample_returns_df[ticker].dropna()

        rolling_predictions = []

        # Initialize the rolling window with in-sample data
        historical_data = returns_in_sample.copy()

        for t in range(len(returns_out_of_sample)):
            # Use only the last rolling_window data points
            train_data = historical_data[-rolling_window:].dropna()

            # Calculate historical VaR
            var = np.percentile(train_data, alpha * 100)
            rolling_predictions.append(var)

            # Update the rolling window with new data
            historical_data = pd.concat([historical_data, returns_out_of_sample.iloc[t:t+1]]).iloc[-rolling_window:]

        # Align forecasts with out-of-sample dates
        prediction_index = returns_out_of_sample.index[:len(rolling_predictions)]
        VaR_predictions = pd.Series(rolling_predictions, index=prediction_index, name="VaR")

        aligned_returns = returns_out_of_sample.reindex(VaR_predictions.index)
        violations_hist_var = (aligned_returns < VaR_predictions).astype(int)

        historical_var_predictions[ticker] = {
            "VaR": VaR_predictions,
            "aligned_returns": aligned_returns,
        }

        pnl_var_df = pd.DataFrame({
            "PnL": aligned_returns,
            "VaR": VaR_predictions,
            "Violations": violations_hist_var
        })

        # Backtesting results
        kupiec_result_hist_var = kupiec_test(pnl_var_df["Violations"], confidence_level)
        berkowitz_result_hist_var = berkowitz_tail_test(pnl_var_df[["PnL"]], volatility_window=252, var_conf_level=1-confidence_level, conf_level=0.99)
        failure_rate_hist_var = violations_hist_var.mean()
        christoffersen_result_hist_var = christoffersen_test(aligned_returns, violations_hist_var, confidence_level)
        engle_manganelli_result_hist_var = engle_manganelli_test(aligned_returns, VaR_predictions, alpha, lags=2)
        tuff_result_hist_var = tuff_test(violations_hist_var, alpha)

        backtest_results[ticker] = {
            'kupiec_result': kupiec_result_hist_var,
            'berkowitz_result': berkowitz_result_hist_var,
            'failure_rate': failure_rate_hist_var,
            'christoffersen_result': christoffersen_result_hist_var,
            'engle_manganelli_result': engle_manganelli_result_hist_var,
            'tuff_result': tuff_result_hist_var
        }

        # Plot returns vs VaR with exceedances
        plt.figure(figsize=(9, 4.5))
        plt.plot(aligned_returns.index, aligned_returns, label="Returns", color="blue", alpha=0.6)
        plt.plot(VaR_predictions.index, VaR_predictions, label=f"VaR ({int(confidence_level * 100)}%)", color="red", linestyle="--")
        plt.fill_between(VaR_predictions.index, VaR_predictions, -0.4, color="red", alpha=0.2, label="Exceedance Zone")
        exceedance_points = pnl_var_df.index[pnl_var_df["Violations"] == 1]
        plt.scatter(exceedance_points, pnl_var_df["PnL"][pnl_var_df["Violations"] == 1], color="black", label="Exceedances", zorder=5)
        plt.title(f"VaR and Returns for {ticker} (Out-of-Sample)")
        plt.xlabel("Date")
        plt.ylabel("Returns / VaR")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    return historical_var_predictions, backtest_results

historical_var_results, backtest_results = rolling_historical_var_forecast(returns_df, out_of_sample_returns_df, rolling_window=1000, confidence_level=0.99)

for ticker, results in backtest_results.items():
    print(f"\nBacktesting results for {ticker}:")
    print(f"Kupiec Test: {results['kupiec_result']}")
    print(f"Berkowitz Test: {results['berkowitz_result']}")
    print(f"Failure Rate: {results['failure_rate']:.2%}")
    print(f"Christoffersen Test: {results['christoffersen_result']}")
    print(f"Engle Manganelli Test: {results['engle_manganelli_result']}")
    print(f"TUFF Test: {results['tuff_result']}")

"""## Backtesting EVT VaR"""

from scipy.stats import genextreme

# Parameters
rolling_window_size = 252  # Length of the rolling window
confidence_level = 0.99
alpha = 1 - confidence_level

# Data out-of-sample
out_of_sample_returns = (prices_out_of_sample.values[1:] / prices_out_of_sample.values[:-1]) - 1
out_of_sample_returns_df = pd.DataFrame(
    out_of_sample_returns, columns=prices_out_of_sample.columns, index=prices_out_of_sample.index[1:]
)
out_of_sample_returns_df= out_of_sample_returns_df.iloc[:300]

def rolling_evt_var_forecast(returns_df, out_of_sample_returns_df, block_size=50, rolling_window=252, confidence_level=0.99):
    evt_var_predictions = {}
    backtest_results = {}

    for ticker in out_of_sample_returns_df.columns:
        print(f"\nRunning Rolling Window EVT VaR for {ticker}...")

        returns_in_sample = returns_df[ticker].dropna().iloc[:-300]  # Use only historical in-sample data
        returns_out_of_sample = out_of_sample_returns_df[ticker].dropna()

        rolling_predictions = []

        # Initialize the rolling window with in-sample data
        historical_data = returns_in_sample.copy()

        for t in range(len(returns_out_of_sample)):
            # Use only the last rolling_window data points
            train_data = historical_data[-rolling_window:].dropna()

            # # Fit GEV to block maxima
            block_size = 21
            block_maxima = []
            for start_idx in range(0, len(train_data), block_size):
                block = -train_data[start_idx : start_idx + block_size]
                block_maxima.append(block.max())

            shape, loc, scale = genextreme.fit(block_maxima)

            # Calculate EVT-based VaR
            if abs(shape) < 1e-4:
                quantile = loc - scale * np.log(-np.log(1 - alpha))
            else:
                quantile = loc - (scale / shape) * (1 - (-np.log(1 - alpha)) ** (-shape))

            daily_evt_var = quantile
            rolling_predictions.append(-daily_evt_var)

            # Update the rolling window with new data
            historical_data = pd.concat([historical_data, returns_out_of_sample.iloc[t:t+1]]).iloc[-rolling_window:]

        # Align forecasts with out-of-sample dates
        prediction_index = returns_out_of_sample.index[:len(rolling_predictions)]
        VaR_predictions = pd.Series(rolling_predictions, index=prediction_index, name="VaR")

        aligned_returns = returns_out_of_sample.reindex(VaR_predictions.index)
        violations_evt_var = (aligned_returns < VaR_predictions).astype(int)


        evt_var_predictions[ticker] = {
            "VaR": VaR_predictions,
            "aligned_returns": aligned_returns,
        }
        pnl_var_df = pd.DataFrame({
            "PnL": aligned_returns,
            "VaR": VaR_predictions,
            "Violations": violations_evt_var
        })

        # Backtesting results
        kupiec_result_evt_var = kupiec_test(pnl_var_df["Violations"], confidence_level)
        berkowitz_result_evt_var = berkowitz_tail_test(pnl_var_df[["PnL"]], volatility_window=252, var_conf_level=1-confidence_level, conf_level=0.99)
        failure_rate_evt_var = violations_evt_var.mean()
        christoffersen_result_evt_var = christoffersen_test(aligned_returns, violations_evt_var, confidence_level)
        engle_manganelli_result_evt_var = engle_manganelli_test(aligned_returns, VaR_predictions, alpha, lags=2)
        tuff_result_evt_var = tuff_test(violations_evt_var, alpha)

        backtest_results[ticker] = {
            'kupiec_result': kupiec_result_evt_var,
            'berkowitz_result': berkowitz_result_evt_var,
            'failure_rate': failure_rate_evt_var,
            'christoffersen_result': christoffersen_result_evt_var,
            'engle_manganelli_result': engle_manganelli_result_evt_var,
            'tuff_result': tuff_result_evt_var
        }

        # Plot returns vs VaR with exceedances
        plt.figure(figsize=(9, 4.5))
        plt.plot(aligned_returns.index, aligned_returns, label="Returns", color="blue", alpha=0.6)
        plt.plot(VaR_predictions.index, VaR_predictions, label=f"VaR ({int(confidence_level * 100)}%)", color="red", linestyle="--")
        plt.fill_between(VaR_predictions.index, VaR_predictions, -0.4, color="red", alpha=0.2, label="Exceedance Zone")
        exceedance_points = pnl_var_df.index[pnl_var_df["Violations"] == 1]
        plt.scatter(exceedance_points, pnl_var_df["PnL"][pnl_var_df["Violations"] == 1], color="black", label="Exceedances", zorder=5)
        plt.title(f"VaR and Returns for {ticker} (Out-of-Sample)")
        plt.xlabel("Date")
        plt.ylabel("Returns / VaR")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    return evt_var_predictions, backtest_results

evt_var_results, backtest_results = rolling_evt_var_forecast(returns_df, out_of_sample_returns_df, block_size=21, rolling_window=1000, confidence_level=0.99)

for ticker, results in backtest_results.items():
    print(f"\nBacktesting results for {ticker}:")
    print(f"Kupiec Test: {results['kupiec_result']}")
    print(f"Berkowitz Test: {results['berkowitz_result']}")
    print(f"Failure Rate: {results['failure_rate']:.2%}")
    print(f"Christoffersen Test: {results['christoffersen_result']}")
    print(f"Engle Manganelli Test: {results['engle_manganelli_result']}")
    print(f"TUFF Test: {results['tuff_result']}")

"""## Backtesting generalized pareto VaR"""

from scipy.stats import genpareto

def var_pareto_gpd(returns, threshold, shape, scale, loc, confidence_level):

    T = len(returns)

    excesses = returns[returns > threshold]
    Tu = len(excesses)  # Number of exceedances

    # print(f"Threshold (u): {threshold}")
    # print(f"Shape (ξ): {shape}")
    # print(f"Scale (σ): {scale}")
    # print(f"Loc (μ): {loc}")
    # print(f"Total Observations (T): {T}")
    # print(f"Exceedances (T_u): {Tu}")

    alpha = 1 - confidence_level
    if Tu == 0:
        raise ValueError("No exceedances found. Adjust the threshold.")

    if shape != 0:
        var = threshold + (scale / shape) * (1 - ((T / Tu * alpha) ** (-shape)))

    else:  # limit case for  shape = 0
        var = threshold - scale - np.log(T / Tu * alpha)

    # print(f"Calculated VaR: {var}")
    return var

# Parameters
# rolling_window_size = 252  # Length of the rolling window
confidence_level = 0.99
alpha = 1 - confidence_level

# Data out-of-sample
out_of_sample_returns = (prices_out_of_sample.values[1:] / prices_out_of_sample.values[:-1]) - 1
out_of_sample_returns_df = pd.DataFrame(
    out_of_sample_returns, columns=prices_out_of_sample.columns, index=prices_out_of_sample.index[1:]
)
out_of_sample_returns_df = out_of_sample_returns_df.iloc[:300]

def rolling_pareto_var_forecast(returns_df, out_of_sample_returns_df, rolling_window=252, confidence_level=0.99):
    pareto_var_predictions = {}
    backtest_results = {}

    for ticker in out_of_sample_returns_df.columns:
        print(f"\nRunning Rolling Window Pareto VaR for {ticker}...")

        returns_in_sample = returns_df[ticker].dropna().iloc[:-300]  # Use only historical in-sample data
        returns_out_of_sample = out_of_sample_returns_df[ticker].dropna()

        rolling_predictions = []

        # Initialize the rolling window with in-sample data
        historical_data = returns_in_sample.copy()

        for t in range(len(returns_out_of_sample)):
            # Use only the last rolling_window data points
            train_data = historical_data[-rolling_window:].dropna()

            # fit Generalized Pareto to exceedances above the threshold
            threshold = np.percentile(train_data, 99)
            excesses = train_data[train_data > threshold] - threshold
            # excesses = threshold - train_data[train_data < threshold]
            params = genpareto.fit(excesses)
            shape, loc, scale = params

            # Calculate Pareto-based VaR
            # var_pareto = threshold + (scale / shape) * ((rolling_window / len(excesses)) ** shape - 1)
            var_pareto = var_pareto_gpd(train_data, threshold, shape, scale, loc, confidence_level)
            rolling_predictions.append(-var_pareto)

            # Update the rolling window with new data
            historical_data = pd.concat([historical_data, returns_out_of_sample.iloc[t:t+1]]).iloc[-rolling_window:]

        # align forecasts with out-of-sample dates
        prediction_index = returns_out_of_sample.index[:len(rolling_predictions)]
        VaR_predictions = pd.Series(rolling_predictions, index=prediction_index, name="VaR")

        # pareto_var_predictions[ticker] = {
        #     "VaR": VaR_predictions
        # }

        aligned_returns = returns_out_of_sample.reindex(VaR_predictions.index)
        violations_pareto_var = (aligned_returns < VaR_predictions).astype(int)


        pareto_var_predictions[ticker] = {
            "VaR": VaR_predictions,
            "aligned_returns": aligned_returns,
        }
        pnl_var_df = pd.DataFrame({
            "PnL": aligned_returns,
            "VaR": VaR_predictions,
            "Violations": violations_pareto_var
        })

        # Backtesting results
        kupiec_result_pareto_var = kupiec_test(pnl_var_df["Violations"], confidence_level)
        berkowitz_result_pareto_var = berkowitz_tail_test(pnl_var_df[["PnL"]], volatility_window=252, var_conf_level=1-confidence_level, conf_level=0.99)
        failure_rate_pareto_var = violations_pareto_var.mean()
        christoffersen_result_pareto_var = christoffersen_test(aligned_returns, violations_pareto_var, confidence_level)
        engle_manganelli_result_pareto_var = engle_manganelli_test(aligned_returns, VaR_predictions, alpha, lags=2)
        tuff_result_pareto_var = tuff_test(violations_pareto_var, alpha)

        backtest_results[ticker] = {
            'kupiec_result': kupiec_result_pareto_var,
            'berkowitz_result': berkowitz_result_pareto_var,
            'failure_rate': failure_rate_pareto_var,
            'christoffersen_result': christoffersen_result_pareto_var,
            'engle_manganelli_result': engle_manganelli_result_pareto_var,
            'tuff_result': tuff_result_pareto_var
        }

        # Plot returns vs VaR with exceedances
        plt.figure(figsize=(9, 4.5))
        plt.plot(aligned_returns.index, aligned_returns, label="Returns", color="blue", alpha=0.6)
        plt.plot(VaR_predictions.index, VaR_predictions, label=f"VaR ({int(confidence_level * 100)}%)", color="red", linestyle="--")
        plt.fill_between(VaR_predictions.index, VaR_predictions, -0.4, color="red", alpha=0.2, label="Exceedance Zone")
        exceedance_points = pnl_var_df.index[pnl_var_df["Violations"] == 1]
        plt.scatter(exceedance_points, pnl_var_df["PnL"][pnl_var_df["Violations"] == 1], color="black", label="Exceedances", zorder=5)
        plt.title(f"VaR and Returns for {ticker} (Out-of-Sample)")
        plt.xlabel("Date")
        plt.ylabel("Returns / VaR")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    return pareto_var_predictions, backtest_results

pareto_var_results, backtest_results = rolling_pareto_var_forecast(returns_df, out_of_sample_returns_df, rolling_window=1000, confidence_level=0.99)

for ticker, results in backtest_results.items():
    print(f"\nBacktesting results for {ticker}:")
    print(f"Kupiec Test: {results['kupiec_result']}")
    print(f"Berkowitz Test: {results['berkowitz_result']}")
    print(f"Failure Rate: {results['failure_rate']:.2%}")
    print(f"Christoffersen Test: {results['christoffersen_result']}")
    print(f"Engle Manganelli Test: {results['engle_manganelli_result']}")
    print(f"TUFF Test: {results['tuff_result']}")

"""## Backtesting GARCH VaR"""

from scipy.stats import t as student_t
from arch import arch_model

def preprocess_train_data(train_data):
    """Preprocess train data by handling NaN and infinite values."""
    if train_data.isnull().any():
        print("Warning: NaN values found in train_data. Filling with median.")
        train_data = train_data.fillna(train_data.median())

    if np.isinf(train_data).any():
        print("Warning: Inf values found in train_data. Replacing with max finite value.")
        train_data.replace([np.inf, -np.inf], np.nan, inplace=True)
        train_data.fillna(train_data.median(), inplace=True)

    return train_data

garch_predictions = {}
backtest_results = {}
# Data out-of-sample
out_of_sample_returns = (prices_out_of_sample.values[1:] / prices_out_of_sample.values[:-1]) - 1
out_of_sample_returns_df = pd.DataFrame(
    out_of_sample_returns, columns=prices_out_of_sample.columns, index=prices_out_of_sample.index[1:]
)
out_of_sample_returns_df= out_of_sample_returns_df.iloc[:300]
# Parameters
rolling_window_size = 252 # lenght of window
confidence_level = 0.99
alpha = 1 - confidence_level


def rolling_garch_forecast_fixed(returns_df, out_of_sample_returns_df, rolling_window=250, confidence_level=0.99):
    garch_predictions = {}
    backtest_results = {}

    for ticker in out_of_sample_returns_df.columns:
        print(f"\nRunning Rolling Window GARCH for {ticker}...")

        returns_in_sample = returns_df[ticker].dropna().iloc[:-300]  # Solo dati in-sample storici
        returns_out_of_sample = out_of_sample_returns_df[ticker].dropna() #.iloc[:rolling_window]

        rolling_predictions = []
        rolling_volatility = []

        # Inizializzazione dei dati storici con i dati in-sample
        historical_data = returns_in_sample.copy()

        # Rolling window GARCH predictions without using future out-of-sample data
        for t in range(len(returns_out_of_sample)):

            # only use data available up to time t keeping fixed rolling window
            train_data = historical_data[-rolling_window:].dropna()

            # Control for nan or inf values
            train_data = preprocess_train_data(train_data)

            #  GARCH model on data available up to time t-1
            model = arch_model(train_data, mean='Zero', vol='Garch', p=1, q=1, dist='t', rescale=False)
            res = model.fit(disp='off')

            # volatility forecast for the next day
            forecast = res.forecast(horizon=1)
            vol = np.sqrt(forecast.variance.iloc[-1, 0])
            rolling_volatility.append(vol)

            # quantile of the t student distribution
            dof = res.params['nu']  # degrees of freedom for t student
            quantile_t = student_t.ppf(1 - confidence_level, df=dof)
            rolling_predictions.append(-abs(vol * quantile_t))

            # update of  rolling window with new data and keeping size fixed
            pred_series = pd.Series(rolling_predictions)
            historical_data = pd.concat([historical_data, returns_out_of_sample.iloc[t:t+1]]).iloc[-rolling_window:]


        # align forecasts with  out-of-sample
        prediction_index = returns_out_of_sample.index[:len(rolling_predictions)]
        VaR_predictions = pd.Series(rolling_predictions, index=prediction_index, name="VaR")
        volatility_predictions = pd.Series(rolling_volatility, index=prediction_index, name="Volatility")

        aligned_returns = returns_out_of_sample.reindex(VaR_predictions.index)
        violations_garch = (aligned_returns < VaR_predictions).astype(int)

        garch_predictions[ticker] = {
            "VaR": VaR_predictions,
            "Volatility": volatility_predictions,
            "aligned_returns": aligned_returns
        }

        pnl_var_df = pd.DataFrame({
            "PnL": aligned_returns,
            "VaR": VaR_predictions,
            "Violations": violations_garch
        })

        print(f"In-sample data points: {len(returns_in_sample)}")
        print(f"Out-of-sample data points: {len(returns_out_of_sample)}")
        print(f"Length of returns: {len(aligned_returns)}")
        print(f"Length of VaR predictions: {len(VaR_predictions)}")

        kupiec_result_garch = kupiec_test(pnl_var_df["Violations"], confidence_level)
        berkowitz_result_garch  = berkowitz_tail_test(pnl_var_df[["PnL"]], volatility_window=252, var_conf_level=1-confidence_level, conf_level=0.99)
        failure_rate_garch  = violations_garch.mean()
        christoffersen_result_garch  = christoffersen_test(aligned_returns, violations_garch, confidence_level)
        engle_manganelli_result_garch  = engle_manganelli_test(aligned_returns, VaR_predictions, alpha, lags=2)
        tuff_result_garch  = tuff_test(violations_garch, alpha)

        backtest_results[ticker] = {
            'kupiec_result': kupiec_result_garch ,
            'berkowitz_result': berkowitz_result_garch ,
            'failure_rate': failure_rate_garch ,
            'christoffersen_result': christoffersen_result_garch ,
            'engle_manganelli_result': engle_manganelli_result_garch ,
            'tuff_result': tuff_result_garch
        }

        # Plot returns vs VaR with exceedances
        plt.figure(figsize=(9, 4.5))
        plt.plot(aligned_returns.index, aligned_returns, label="Returns", color="blue", alpha=0.6)
        plt.plot(VaR_predictions.index, VaR_predictions, label=f"VaR ({int(confidence_level * 100)}%)", color="red", linestyle="--")
        plt.fill_between(VaR_predictions.index, VaR_predictions, -0.4, color="red", alpha=0.2, label="Exceedance Zone")
        exceedance_points = pnl_var_df.index[pnl_var_df["Violations"] == 1]
        plt.scatter(exceedance_points, pnl_var_df["PnL"][pnl_var_df["Violations"] == 1], color="black", label="Exceedances", zorder=5)
        plt.title(f"VaR and Returns for {ticker} (Out-of-Sample)")
        plt.xlabel("Date")
        plt.ylabel("Returns / VaR")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

        # Plot realized vs predicted volatility
        realized_volatility = aligned_returns.abs()
        plt.figure(figsize=(7, 3))
        plt.plot(realized_volatility.index, realized_volatility, label="Realized Volatility", color="blue", alpha=0.6)
        plt.plot(volatility_predictions.index, volatility_predictions, label="Predicted Volatility", color="orange", linestyle="--")
        plt.title(f"Realized vs Predicted Volatility for {ticker}")
        plt.xlabel("Date")
        plt.ylabel("Volatility")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    return garch_predictions, backtest_results

garch_results, backtest_results = rolling_garch_forecast_fixed(returns_df, out_of_sample_returns_df, rolling_window=1000, confidence_level=0.99)


for ticker, results in backtest_results.items():
    print(f"\nBacktesting results for {ticker}:")
    print(f"Kupiec Test: {results['kupiec_result']}")
    print(f"Berkowitz Test: {results['berkowitz_result']}")
    print(f"Failure Rate: {results['failure_rate']:.2%}")
    print(f"Christoffersen Test: {results['christoffersen_result']}")
    print(f"Engle Manganelli Test: {results['engle_manganelli_result']}")

    tuff_result_garch  = results['tuff_result']
    print(f"TUFF Test: Time Until First Failure = {tuff_result_garch['time_to_failure']}, "
          f"Likelihood Ratio = {tuff_result_garch['likelihood_ratio']:.6f}, "
          f"P-value = {tuff_result_garch['p_value']:.6f}, Decision = {tuff_result_garch['decision']}")

"""## Backtesting of Garch with expanding window"""

from scipy.stats import t as student_t
from arch import arch_model
def preprocess_train_data(train_data):
    """Preprocess train data by handling NaN and infinite values."""
    if train_data.isnull().any():
        print("Warning: NaN values found in train_data. Filling with median.")
        train_data = train_data.fillna(train_data.median())

    if np.isinf(train_data).any():
        print("Warning: Inf values found in train_data. Replacing with max finite value.")
        train_data.replace([np.inf, -np.inf], np.nan, inplace=True)
        train_data.fillna(train_data.median(), inplace=True)

    return train_data

garch_predictions = {}
backtest_results = {}
# Data out-of-sample
out_of_sample_returns = (prices_out_of_sample.values[1:] / prices_out_of_sample.values[:-1]) - 1
out_of_sample_returns_df = pd.DataFrame(
    out_of_sample_returns, columns=prices_out_of_sample.columns, index=prices_out_of_sample.index[1:]
)
out_of_sample_returns_df= out_of_sample_returns_df.iloc[:300]
# Parameters
rolling_window_size = 252 # lenght of window
confidence_level = 0.99
alpha = 1 - confidence_level


def expanding_garch_forecast(returns_df, out_of_sample_returns_df, confidence_level=0.99):
    garch_predictions = {}
    backtest_results = {}

    for ticker in out_of_sample_returns_df.columns:
        print(f"\nRunning Expanding Window GARCH for {ticker}...")

        returns_in_sample = returns_df[ticker].dropna().iloc[:-300]  # Solo dati in-sample storici
        returns_out_of_sample = out_of_sample_returns_df[ticker].dropna()

        expanding_predictions = []
        expanding_volatility = []

        # Inizializzazione dei dati storici con i dati in-sample
        historical_data = returns_in_sample.copy()

        # Expanding window GARCH predictions
        for t in range(len(returns_out_of_sample)):

            # Usa tutti i dati disponibili fino a t (expanding window)
            train_data = historical_data.dropna()

            # Controllo per NaN o inf
            train_data = preprocess_train_data(train_data)

            # Modello GARCH(1,1)
            model = arch_model(train_data, mean='Zero', vol='Garch', p=1, q=1, dist='t', rescale=False)
            res = model.fit(disp='off')

            # Previsione della volatilità per il giorno successivo
            forecast = res.forecast(horizon=1)
            vol = np.sqrt(forecast.variance.iloc[-1, 0])
            expanding_volatility.append(vol)

            # Calcolo del quantile della distribuzione t-student
            dof = res.params['nu']
            quantile_t = student_t.ppf(1 - confidence_level, df=dof)
            expanding_predictions.append(-abs(vol * quantile_t))

            # Aggiungi il nuovo dato alla finestra espandibile
            historical_data = pd.concat([historical_data, returns_out_of_sample.iloc[t:t+1]])

        # Allinea le previsioni con i dati out-of-sample
        prediction_index = returns_out_of_sample.index[:len(expanding_predictions)]
        VaR_predictions = pd.Series(expanding_predictions, index=prediction_index, name="VaR")
        volatility_predictions = pd.Series(expanding_volatility, index=prediction_index, name="Volatility")

        aligned_returns = returns_out_of_sample.reindex(VaR_predictions.index)
        violations_garch = (aligned_returns < VaR_predictions).astype(int)

        garch_predictions[ticker] = {
            "VaR": VaR_predictions,
            "Volatility": volatility_predictions,
            "aligned_returns": aligned_returns
        }

        pnl_var_df = pd.DataFrame({
            "PnL": aligned_returns,
            "VaR": VaR_predictions,
            "Violations": violations_garch
        })

        kupiec_result_garch = kupiec_test(pnl_var_df["Violations"], confidence_level)
        berkowitz_result_garch = berkowitz_tail_test(pnl_var_df[["PnL"]], volatility_window=252, var_conf_level=1-confidence_level, conf_level=0.99)
        failure_rate_garch = violations_garch.mean()
        christoffersen_result_garch = christoffersen_test(aligned_returns, violations_garch, confidence_level)
        engle_manganelli_result_garch = engle_manganelli_test(aligned_returns, VaR_predictions, 1-confidence_level, lags=2)
        tuff_result_garch = tuff_test(violations_garch, 1-confidence_level)

        backtest_results[ticker] = {
            'kupiec_result': kupiec_result_garch,
            'berkowitz_result': berkowitz_result_garch,
            'failure_rate': failure_rate_garch,
            'christoffersen_result': christoffersen_result_garch,
            'engle_manganelli_result': engle_manganelli_result_garch,
            'tuff_result': tuff_result_garch
        }

        # Grafici delle previsioni
        plt.figure(figsize=(9, 4.5))
        plt.plot(aligned_returns.index, aligned_returns, label="Returns", color="blue", alpha=0.6)
        plt.plot(VaR_predictions.index, VaR_predictions, label=f"VaR ({int(confidence_level * 100)}%)", color="red", linestyle="--")
        plt.fill_between(VaR_predictions.index, VaR_predictions, -0.4, color="red", alpha=0.2, label="Exceedance Zone")
        exceedance_points = pnl_var_df.index[pnl_var_df["Violations"] == 1]
        plt.scatter(exceedance_points, pnl_var_df["PnL"][pnl_var_df["Violations"] == 1], color="black", label="Exceedances", zorder=5)
        plt.title(f"VaR and Returns for {ticker} (Expanding Window Out-of-Sample)")
        plt.xlabel("Date")
        plt.ylabel("Returns / VaR")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

        # Grafico della volatilità realizzata vs prevista
        realized_volatility = aligned_returns.abs()
        plt.figure(figsize=(7, 3))
        plt.plot(realized_volatility.index, realized_volatility, label="Realized Volatility", color="blue", alpha=0.6)
        plt.plot(volatility_predictions.index, volatility_predictions, label="Predicted Volatility", color="orange", linestyle="--")
        plt.title(f"Realized vs Predicted Volatility for {ticker}")
        plt.xlabel("Date")
        plt.ylabel("Volatility")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    return garch_predictions, backtest_results



garch_results, backtest_results = expanding_garch_forecast(returns_df, out_of_sample_returns_df, confidence_level=0.99)


for ticker, results in backtest_results.items():
    print(f"\nBacktesting results for {ticker}:")
    print(f"Kupiec Test: {results['kupiec_result']}")
    print(f"Berkowitz Test: {results['berkowitz_result']}")
    print(f"Failure Rate: {results['failure_rate']:.2%}")
    print(f"Christoffersen Test: {results['christoffersen_result']}")
    print(f"Engle Manganelli Test: {results['engle_manganelli_result']}")

    tuff_result_garch  = results['tuff_result']
    print(f"TUFF Test: Time Until First Failure = {tuff_result_garch['time_to_failure']}, "
          f"Likelihood Ratio = {tuff_result_garch['likelihood_ratio']:.6f}, "
          f"P-value = {tuff_result_garch['p_value']:.6f}, Decision = {tuff_result_garch['decision']}")

"""## Backtesting EVT and GARCH (EVT parameters fixed)"""

from arch import arch_model
from scipy.stats import genextreme


# Data out-of-sample
out_of_sample_returns = (prices_out_of_sample.values[1:] / prices_out_of_sample.values[:-1]) - 1
out_of_sample_returns_df = pd.DataFrame(
    out_of_sample_returns, columns=prices_out_of_sample.columns, index=prices_out_of_sample.index[1:]
)
out_of_sample_returns_df= out_of_sample_returns_df.iloc[:300]
gev_params_fixed = {
    'AMZN': (-0.4214, 2.1795, 0.8888),
    'DIS': (-0.3328, 2.1077, 0.6123),
    'AAPL': (-0.2983, 2.2263, 0.6617),
    'MSFT': (-0.3259, 2.2201, 0.7316),
    'JPM': (-0.0486, 2.2118, 0.5952),
    'F': (-0.1567, 2.0240, 0.6352),
    'PG': (-0.2935, 2.0955, 0.5909),
    '^GSPC': (-0.1493, 1.9445, 0.3304),
    '^IXIC': (-0.0126, 1.9943, 0.3500),
}

rolling_window_size = 1000
confidence_level = 0.99
alpha = 1 - confidence_level

def rolling_garch_evt_forecast_fixed(returns_df, out_of_sample_returns_df, rolling_window=250, confidence_level=0.99):
    garch_evt_predictions = {}
    backtest_results = {}

    for ticker in out_of_sample_returns_df.columns:
        print(f"\nProcessing GARCH+EVT for {ticker}...")

        returns_in_sample = returns_df[ticker].dropna().iloc[:-300]  # Usa solo dati storici in-sample
        returns_out_of_sample = out_of_sample_returns_df[ticker].dropna()
        rolling_predictions = []
        rolling_volatility = []

        # Recupera old EVT parameters
        if ticker in gev_params_fixed:
            shape, loc, scale = gev_params_fixed[ticker]
        else:
            print(f"Warning: No GEV parameters for {ticker}. Skipping.")
            continue

        print(f"Using fixed EVT parameters for {ticker}: Shape={shape:.4f}, Loc={loc:.4f}, Scale={scale:.4f}")

        # Inizializza la rolling window con dati in-sample
        historical_data = returns_in_sample.copy()

        for t in range(len(returns_out_of_sample)):

            # only use the last data
            train_data = historical_data[-rolling_window:].dropna()

            # Control - check for  NaN o inf values
            if train_data.isnull().any():
                print(f"Warning: NaN values found in train_data for {ticker}. Filling with zeros.")
                train_data = train_data.fillna(0)

            if np.isinf(train_data).any():
                print(f"Warning: Inf values found in train_data for {ticker}. Replacing with max finite value.")
                train_data.replace([np.inf, -np.inf], np.nan, inplace=True)
                train_data.fillna(train_data.max(), inplace=True)

            # garch fitted up to  t-1
            model = arch_model(train_data, mean='Zero', vol='Garch', p=1, q=1, dist='t', rescale=False)
            res = model.fit(disp='off')

            # forecast for next day volatility
            forecast = res.forecast(horizon=1)
            vol = np.sqrt(forecast.variance.iloc[-1, 0])
            rolling_volatility.append(vol)

            # compute evt quantile
            if abs(shape) < 1e-6:
                quantile = loc - scale * np.log(-np.log(1 - alpha))
            else:
                quantile = loc - (scale / shape) * (1 - (-np.log(1 - alpha)) ** (-shape))
            # print(f"quantile: {quantile}")
            daily_gev_var = vol * quantile
            rolling_predictions.append(-daily_gev_var)

            pred_series = pd.Series(rolling_predictions)
            # update rolling window
            historical_data = pd.concat([historical_data, returns_out_of_sample.iloc[t:t+1]]).iloc[-rolling_window:]


        # align forecasts with  with date out-of-sample
        prediction_index = returns_out_of_sample.index
        VaR_predictions = pd.Series(rolling_predictions, index=prediction_index[:len(rolling_predictions)], name="VaR")
        volatility_predictions = pd.Series(rolling_volatility, index=prediction_index[:len(rolling_volatility)], name="Volatility")
        aligned_returns = returns_out_of_sample.reindex(VaR_predictions.index)

        garch_evt_predictions[ticker] = {
            "VaR": VaR_predictions,
            "Volatility": volatility_predictions,
            "aligned_returns": aligned_returns
        }

        violations_gevgarch = (returns_out_of_sample < VaR_predictions).astype(int)
        pnl_var_df = pd.DataFrame({
            "PnL": returns_out_of_sample,
            "VaR": VaR_predictions,
            "Violations": violations_gevgarch
        })

        kupiec_result_gevgarch = kupiec_test(pnl_var_df["Violations"], confidence_level)
        christoffersen_result_gevgarch = christoffersen_test(returns_out_of_sample, violations_gevgarch, confidence_level)
        engle_manganelli_result_gevgarch = engle_manganelli_test(returns_out_of_sample, VaR_predictions, alpha, lags=2)
        tuff_result_gevgarch = tuff_test(violations_gevgarch, alpha)
        berkowitz_result_gevgarch = berkowitz_tail_test(pnl_var_df[["PnL"]], volatility_window=252, var_conf_level=1-confidence_level, conf_level=0.99)

        backtest_results[ticker] = {
            'kupiec_result': kupiec_result_gevgarch,
            'berkowitz_result': berkowitz_result_gevgarch,
            'christoffersen_result': christoffersen_result_gevgarch,
            'engle_manganelli_result': engle_manganelli_result_gevgarch,
            'tuff_result': tuff_result_gevgarch,
            'failure_rate': violations_gevgarch.mean()
        }

        # Plot returns vs VaR with exceedances
        plt.figure(figsize=(9, 4.5))
        plt.plot(returns_out_of_sample.index, returns_out_of_sample, label="Returns", color="blue", alpha=0.6)
        plt.plot(VaR_predictions.index, VaR_predictions, label=f"VaR ({int(confidence_level * 100)}%)", color="red", linestyle="--")
        plt.fill_between(VaR_predictions.index, VaR_predictions, -0.4, color="red", alpha=0.2, label="Exceedance Zone")
        exceedance_points = pnl_var_df.index[pnl_var_df["Violations"] == 1]
        plt.scatter(exceedance_points, pnl_var_df["PnL"][pnl_var_df["Violations"] == 1], color="black", label="Exceedances", zorder=5)
        plt.title(f"VaR and Returns for {ticker} (Forecast Out-of-Sample)")
        plt.xlabel("Date")
        plt.ylabel("Returns / VaR")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    return garch_evt_predictions, backtest_results

garch_evt_results, backtest_results = rolling_garch_evt_forecast_fixed(returns_df, out_of_sample_returns_df, rolling_window=500, confidence_level=0.99)

for ticker, results in backtest_results.items():
    print(f"\nBacktesting results for {ticker}:")
    print(f"Kupiec Test: {results['kupiec_result']}")
    print(f"Berkowitz Test: {results['berkowitz_result']}")
    print(f"Tuff Test: {results['tuff_result']}")
    print(f"Engle and Manganelli Test: {results['engle_manganelli_result']}")
    print(f"Christoffersen Test: {results['christoffersen_result']}")
    print(f"Failure Rate: {results['failure_rate']:.2%}")

"""## Backtest for EVT+GARCH with a Different approach - rolling window that also refits the evt model, no fixed parameters"""

from arch import arch_model
from scipy.stats import genextreme


out_of_sample_returns = (prices_out_of_sample.values[1:] / prices_out_of_sample.values[:-1]) - 1
out_of_sample_returns_df = pd.DataFrame(
    out_of_sample_returns, columns=prices_out_of_sample.columns, index=prices_out_of_sample.index[1:]
)
out_of_sample_returns_df= out_of_sample_returns_df.iloc[:300]

rolling_window_size = 1000
confidence_level = 0.99
alpha = 1 - confidence_level

def rolling_garch_evt_forecast_dynamic(returns_df, out_of_sample_returns_df, rolling_window=250, block_size=21, confidence_level=0.99):
    garch_evt_predictions = {}
    backtest_results = {}

    alpha = 1 - confidence_level  # Livello di significatività per il VaR

    for ticker in out_of_sample_returns_df.columns:
        print(f"\nProcessing GARCH+EVT with Dynamic Parameters for {ticker}...")

        returns_in_sample = returns_df[ticker].dropna().iloc[:-300]  # Dati storici in-sample
        returns_out_of_sample = out_of_sample_returns_df[ticker].dropna()

        rolling_predictions = []
        rolling_volatility = []

        historical_data = returns_in_sample.copy()

        for t in range(len(returns_out_of_sample)):
            train_data = historical_data[-rolling_window:].dropna()

            # Controllo dei dati mancanti o infiniti
            if train_data.isnull().any():
                train_data = train_data.fillna(0)
            if np.isinf(train_data).any():
                train_data.replace([np.inf, -np.inf], np.nan, inplace=True)
                train_data.fillna(train_data.max(), inplace=True)

            # Ristima del modello GARCH ogni giorno
            garch_model = arch_model(train_data, mean='Zero', vol='Garch', p=1, q=1, dist='t', rescale=False)
            garch_result = garch_model.fit(disp='off')

            # Previsione della volatilità per il giorno successivo
            forecast = garch_result.forecast(horizon=1)
            vol_forecast = np.sqrt(forecast.variance.iloc[-1, 0])
            rolling_volatility.append(vol_forecast)

            # Calcolo dei residui del GARCH (innovazioni standardizzate)
            residuals = garch_result.resid / garch_result.conditional_volatility
            residuals = residuals.dropna()

            # Estrazione dei massimi dei blocchi dai residui per la EVT
            block_maxima = []
            for start_idx in range(0, len(residuals), block_size):
                block = residuals[start_idx : start_idx + block_size]
                if not block.empty:
                    block_maxima.append(block.max())

            # Verifica se ci sono abbastanza dati per stimare la EVT
            if len(block_maxima) < 5:
                print(f"Not enough extreme values for EVT estimation for {ticker} at iteration {t}. Skipping this step.")
                continue

            # Stima dinamica dei parametri EVT (GEV) sui massimi dei blocchi
            shape, loc, scale = genextreme.fit(block_maxima)

            # Calcolo del quantile EVT per il livello di confidenza desiderato
            if abs(shape) < 1e-6:  # Caso limite per distribuzione di Gumbel
                quantile_evt = loc - scale * np.log(-np.log(1 - alpha))
            else:
                quantile_evt = loc - (scale / shape) * (1 - (-np.log(1 - alpha)) ** (-shape))

            # Calcolo del VaR combinando la volatilità GARCH con il quantile EVT
            daily_var = vol_forecast * quantile_evt
            rolling_predictions.append(-daily_var)

            # Aggiornamento della rolling window con i dati out-of-sample
            historical_data = pd.concat([historical_data, returns_out_of_sample.iloc[t:t+1]]).iloc[-rolling_window:]

        # Allineamento dei risultati con le date out-of-sample
        prediction_index = returns_out_of_sample.index
        VaR_predictions = pd.Series(rolling_predictions, index=prediction_index[:len(rolling_predictions)], name="VaR")
        volatility_predictions = pd.Series(rolling_volatility, index=prediction_index[:len(rolling_volatility)], name="Volatility")

        # Salvataggio dei risultati per ciascun titolo
        garch_evt_predictions[ticker] = {
            "VaR": VaR_predictions,
            "Volatility": volatility_predictions,
            "Returns": returns_out_of_sample.reindex(VaR_predictions.index)
        }

        violations_gevgarch = (returns_out_of_sample < VaR_predictions).astype(int)
        pnl_var_df = pd.DataFrame({
            "PnL": returns_out_of_sample,
            "VaR": VaR_predictions,
            "Violations": violations_gevgarch
        })

        kupiec_result_gevgarch = kupiec_test(pnl_var_df["Violations"], confidence_level)
        christoffersen_result_gevgarch = christoffersen_test(returns_out_of_sample, violations_gevgarch, confidence_level)
        engle_manganelli_result_gevgarch = engle_manganelli_test(returns_out_of_sample, VaR_predictions, alpha, lags=2)
        tuff_result_gevgarch = tuff_test(violations_gevgarch, alpha)
        berkowitz_result_gevgarch = berkowitz_tail_test(pnl_var_df[["PnL"]], volatility_window=252, var_conf_level=1-confidence_level, conf_level=0.99)

        backtest_results[ticker] = {
            'kupiec_result': kupiec_result_gevgarch,
            'berkowitz_result': berkowitz_result_gevgarch,
            'christoffersen_result': christoffersen_result_gevgarch,
            'engle_manganelli_result': engle_manganelli_result_gevgarch,
            'tuff_result': tuff_result_gevgarch,
            'failure_rate': violations_gevgarch.mean()
        }

        # Plot returns vs VaR with exceedances
        plt.figure(figsize=(9, 4.5))
        plt.plot(returns_out_of_sample.index, returns_out_of_sample, label="Returns", color="blue", alpha=0.6)
        plt.plot(VaR_predictions.index, VaR_predictions, label=f"VaR ({int(confidence_level * 100)}%)", color="red", linestyle="--")
        plt.fill_between(VaR_predictions.index, VaR_predictions, -0.4, color="red", alpha=0.2, label="Exceedance Zone")
        exceedance_points = pnl_var_df.index[pnl_var_df["Violations"] == 1]
        plt.scatter(exceedance_points, pnl_var_df["PnL"][pnl_var_df["Violations"] == 1], color="black", label="Exceedances", zorder=5)
        plt.title(f"VaR and Returns for {ticker} (Forecast Out-of-Sample)")
        plt.xlabel("Date")
        plt.ylabel("Returns / VaR")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    return garch_evt_predictions, backtest_results

garch_evt_results, backtest_results = rolling_garch_evt_forecast_dynamic(returns_df, out_of_sample_returns_df, rolling_window=500, confidence_level=0.99)

for ticker, results in backtest_results.items():
    print(f"\nBacktesting results for {ticker}:")
    print(f"Kupiec Test: {results['kupiec_result']}")
    print(f"Berkowitz Test: {results['berkowitz_result']}")
    print(f"Tuff Test: {results['tuff_result']}")
    print(f"Engle and Manganelli Test: {results['engle_manganelli_result']}")
    print(f"Christoffersen Test: {results['christoffersen_result']}")
    print(f"Failure Rate: {results['failure_rate']:.2%}")

"""## Backtesting filtered historical simulation"""

from arch import arch_model
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

# Parameters
rolling_window_size = 252  # Length of the rolling window
n_simulations = 100  # Number of simulations
forecast_horizon = 300  # Time horizon for the forecast
confidence_level = 0.99
alpha = 1 - confidence_level

# Data out-of-sample
out_of_sample_returns = (prices_out_of_sample.values[1:] / prices_out_of_sample.values[:-1]) - 1
out_of_sample_returns_df = pd.DataFrame(
    out_of_sample_returns, columns=prices_out_of_sample.columns, index=prices_out_of_sample.index[1:]
)
out_of_sample_returns_df= out_of_sample_returns_df.iloc[:300]


def rolling_fhs_garch_forecast(returns_df, out_of_sample_returns_df, rolling_window=252, n_simulations=100, forecast_horizon=250, confidence_level=0.99):
    fhs_garch_predictions = {}
    backtest_results = {}

    for ticker in out_of_sample_returns_df.columns:
        print(f"\nRunning Rolling Window FHS-GARCH for {ticker}...")

        returns_in_sample = returns_df[ticker].dropna().iloc[:-300]  # Use only historical in-sample data
        returns_out_of_sample = out_of_sample_returns_df[ticker].dropna()
        rolling_predictions = []
        rolling_volatility = []

        # Initialize the rolling window with in-sample data
        historical_data = returns_in_sample.copy()

        for t in range(len(returns_out_of_sample)):
            # Use only the last rolling_window data points
            train_data = historical_data[-rolling_window:].dropna()

            # Fit GARCH model on data available up to time t-1
            model = arch_model(train_data, mean='Zero', vol='Garch', p=1, q=1, dist='t', rescale=False)
            res = model.fit(disp='off')

            # Forecast volatility for the next day
            forecast = res.forecast(horizon=1)
            vol = np.sqrt(forecast.variance.iloc[-1, 0])
            rolling_volatility.append(vol)

            # Simulate future returns
            z = res.std_resid.dropna()  # Standardized residuals
            simulated_paths = np.zeros((forecast_horizon, n_simulations))
            for sim in range(n_simulations):
                sampled_residuals = np.random.choice(z, size=forecast_horizon, replace=True)
                simulated_returns = sampled_residuals * vol
                simulated_paths[:, sim] = simulated_returns

            # Calculate daily VaR
            daily_VaRs = np.percentile(simulated_paths, (1 - confidence_level) * 100, axis=1)
            rolling_predictions.append(daily_VaRs[0])

            # Update the rolling window with new data
            historical_data = pd.concat([historical_data, returns_out_of_sample.iloc[t:t+1]]).iloc[-rolling_window:]

        # Align forecasts with out-of-sample dates
        prediction_index = returns_out_of_sample.index[:len(rolling_predictions)]
        VaR_predictions = pd.Series(rolling_predictions, index=prediction_index, name="VaR")
        volatility_predictions = pd.Series(rolling_volatility, index=prediction_index, name="Volatility")

        aligned_returns = returns_out_of_sample.reindex(VaR_predictions.index)
        violations_fhs_garch = (aligned_returns < VaR_predictions).astype(int)

        fhs_garch_predictions[ticker] = {
            "VaR": VaR_predictions,
            "Volatility": volatility_predictions,
            "aligned_returns": aligned_returns
        }

        pnl_var_df = pd.DataFrame({
            "PnL": aligned_returns,
            "VaR": VaR_predictions,
            "Violations": violations_fhs_garch
        })

        # Backtesting results
        kupiec_result_fhs_garch = kupiec_test(pnl_var_df["Violations"], confidence_level)
        berkowitz_result_fhs_garch = berkowitz_tail_test(pnl_var_df[["PnL"]], volatility_window=252, var_conf_level=1-confidence_level, conf_level=0.99)
        failure_rate_fhs_garch = violations_fhs_garch.mean()
        christoffersen_result_fhs_garch = christoffersen_test(aligned_returns, violations_fhs_garch, confidence_level)
        engle_manganelli_result_fhs_garch = engle_manganelli_test(aligned_returns, VaR_predictions, alpha, lags=2)
        tuff_result_fhs_garch = tuff_test(violations_fhs_garch, alpha)

        backtest_results[ticker] = {
            'kupiec_result': kupiec_result_fhs_garch,
            'berkowitz_result': berkowitz_result_fhs_garch,
            'failure_rate': failure_rate_fhs_garch,
            'christoffersen_result': christoffersen_result_fhs_garch,
            'engle_manganelli_result': engle_manganelli_result_fhs_garch,
            'tuff_result': tuff_result_fhs_garch
        }

        # Plot returns vs VaR with exceedances
        plt.figure(figsize=(9, 4.5))
        plt.plot(aligned_returns.index, aligned_returns, label="Returns", color="blue", alpha=0.6)
        plt.plot(VaR_predictions.index, VaR_predictions, label=f"VaR ({int(confidence_level * 100)}%)", color="red", linestyle="--")
        plt.fill_between(VaR_predictions.index, VaR_predictions, -0.4, color="red", alpha=0.2, label="Exceedance Zone")
        exceedance_points = pnl_var_df.index[pnl_var_df["Violations"] == 1]
        plt.scatter(exceedance_points, pnl_var_df["PnL"][pnl_var_df["Violations"] == 1], color="black", label="Exceedances", zorder=5)
        plt.title(f"VaR and Returns for {ticker} (Out-of-Sample)")
        plt.xlabel("Date")
        plt.ylabel("Returns / VaR")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

        # Plot realized vs predicted volatility
        realized_volatility = aligned_returns.abs()
        plt.figure(figsize=(7, 3))
        plt.plot(realized_volatility.index, realized_volatility, label="Realized Volatility", color="blue", alpha=0.6)
        plt.plot(volatility_predictions.index, volatility_predictions, label="Predicted Volatility", color="orange", linestyle="--")
        plt.title(f"Realized vs Predicted Volatility for {ticker}")
        plt.xlabel("Date")
        plt.ylabel("Volatility")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    return fhs_garch_predictions, backtest_results

fhs_garch_results, backtest_results = rolling_fhs_garch_forecast(returns_df, out_of_sample_returns_df, rolling_window=1000, n_simulations=100, forecast_horizon=250, confidence_level=0.99)

for ticker, results in backtest_results.items():
    print(f"\nBacktesting results for {ticker}:")
    print(f"Kupiec Test: {results['kupiec_result']}")
    print(f"Berkowitz Test: {results['berkowitz_result']}")
    print(f"Failure Rate: {results['failure_rate']:.2%}")
    print(f"Christoffersen Test: {results['christoffersen_result']}")
    print(f"Engle Manganelli Test: {results['engle_manganelli_result']}")
    print(f"TUFF Test: {results['tuff_result']}")

"""## Backtesting quantile regression"""

import statsmodels.api as sm
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import warnings

def add_lags(data, lags=2):
    lagged_data = data.copy()
    for lag in range(1, lags + 1):
        lagged_data[f'lag_{lag}'] = lagged_data['returns'].shift(lag)
    return lagged_data.dropna()

def add_lags_and_squared(data, lags=1):
    lagged_data = data.copy()
    for lag in range(1, lags + 1):
        lagged_data[f'lag_{lag}'] = lagged_data['returns'].shift(lag)
        lagged_data[f'lag_{lag}_squared'] = lagged_data[f'lag_{lag}'] ** 2
    return lagged_data.dropna()

def generate_out_of_sample_lags(out_of_sample_row, in_sample_data, lags):
    """
    Generate lagged features for the out-of-sample row based on the latest in-sample data.
    """
    lagged_features = {}
    for lag in range(1, lags + 1):
        lagged_value = in_sample_data['returns'].iloc[-lag] if len(in_sample_data) >= lag else np.nan
        lagged_features[f'lag_{lag}'] = lagged_value
    return lagged_features


# Data out-of-sample
out_of_sample_returns = (prices_out_of_sample.values[1:] / prices_out_of_sample.values[:-1]) - 1
out_of_sample_returns_df = pd.DataFrame(
    out_of_sample_returns, columns=prices_out_of_sample.columns, index=prices_out_of_sample.index[1:]
)
out_of_sample_returns_df= out_of_sample_returns_df.iloc[:300]
backtest_results= {}
warnings.filterwarnings("ignore", category=sm.tools.sm_exceptions.IterationLimitWarning)

def backtest_var(returns_df, out_of_sample_returns_df, alpha=0.01, lags=2, window=250):
    results = {}

    for ticker in returns_df.columns:
        print(f"\nQuantile Regression for {ticker}...")

        # In-sample data
        returns_in_sample = returns_df[ticker].dropna().iloc[:-300]
        returns_in_sample = returns_in_sample.to_frame(name='returns')

        # Out-of-sample data
        returns_out_of_sample = out_of_sample_returns_df[ticker].dropna().iloc[:300]
        returns_out_of_sample = returns_out_of_sample.to_frame(name='returns')

        VaR_dynamic_series = []
        index_series = []
        exceedances = 0
        exceedance_points = []

        violations = []  # Store 1 for exceedances, 0 otherwise

        # Rolling window prediction for out-of-sample data
        for out_of_sample_index, out_of_sample_row in returns_out_of_sample.iterrows():
            lagged_data = add_lags(returns_in_sample, lags=lags)

            if lagged_data.empty:
                continue

            y = lagged_data['returns']
            X = lagged_data.drop(columns=['returns'])
            X = sm.add_constant(X)

            model = sm.QuantReg(y, X)
            try:
                quantile_model = model.fit(q=alpha, max_iter=5000, disp=False)
            except Exception as e:
                warnings.warn(f"Quantile regression failed for {ticker} at index {out_of_sample_index}: {e}")
                continue

            # Generate lagged features for the current out-of-sample observation
            lagged_features = generate_out_of_sample_lags(out_of_sample_row, returns_in_sample, lags)
            X_out_of_sample = [1] + [lagged_features[f'lag_{i}'] for i in range(1, lags + 1)]

            # Predict VaR for the current out-of-sample observation
            try:
                VaR_dynamic = quantile_model.predict([X_out_of_sample])[0]
            except Exception as e:
                warnings.warn(f"Prediction failed for {ticker} at index {out_of_sample_index}: {e}")
                continue

            VaR_dynamic_series.append(VaR_dynamic)
            index_series.append(out_of_sample_index)

            # Check for exceedance
            if out_of_sample_row['returns'] < VaR_dynamic:
                exceedances += 1
                exceedance_points.append(out_of_sample_index)
                violations.append(1)
            else:
                violations.append(0)

            # Update the in-sample data with the current out-of-sample observation
            returns_in_sample = pd.concat([returns_in_sample, out_of_sample_row.to_frame().T])
            returns_in_sample = returns_in_sample.iloc[-window:]  # Keep the window size fixed

        VaR_dynamic_series = pd.Series(VaR_dynamic_series, index=pd.DatetimeIndex(index_series), name='VaR_dynamic')

        aligned_returns = returns_out_of_sample.reindex(VaR_dynamic_series.index).squeeze()

        violations_2 = (returns_out_of_sample["returns"] < VaR_dynamic_series).astype(int)
        # Store results for further analysis
        results[ticker] = {
            "VaR": VaR_dynamic_series,
            "violations": pd.Series(violations, index=pd.DatetimeIndex(index_series)),
            "exceedances": exceedances,
            "violations_2": violations_2,
            "returns_out_of_sample": returns_out_of_sample,
            "aligned_returns": aligned_returns
        }
        alligned_returns_dataframe = returns_out_of_sample["returns"].to_frame()
        kupiec_result_garch = kupiec_test(violations_2, 0.99)
        berkowitz_result_garch  = berkowitz_tail_test(alligned_returns_dataframe, volatility_window=252, var_conf_level=1-0.99, conf_level=0.99)
        failure_rate_garch  = violations_2.mean()
        christoffersen_result_garch  = christoffersen_test(aligned_returns, violations_2, 0.99)
        engle_manganelli_result_garch  = engle_manganelli_test(aligned_returns, VaR_dynamic_series, alpha, lags=2)
        tuff_result_garch  = tuff_test(violations_2, alpha)

        backtest_results[ticker] = {
            'kupiec_result': kupiec_result_garch ,
            'berkowitz_result': berkowitz_result_garch ,
            'failure_rate': failure_rate_garch ,
            'christoffersen_result': christoffersen_result_garch ,
            'engle_manganelli_result': engle_manganelli_result_garch ,
            'tuff_result': tuff_result_garch
        }
        plt.figure(figsize=(8, 4.5))
        plt.plot(returns_out_of_sample.index, returns_out_of_sample['returns'], label="Returns", color="blue", alpha=0.6)
        plt.plot(VaR_dynamic_series.index, VaR_dynamic_series, label=f"Dynamic VaR ({int(alpha*100)}%)", color="red", linestyle="--")
        plt.fill_between(VaR_dynamic_series.index, VaR_dynamic_series, -0.5, color="red", alpha=0.2, label="Exceedance Zone")
        plt.scatter(exceedance_points, returns_out_of_sample.loc[exceedance_points, 'returns'], color='black', label="Exceedances", zorder=5)
        plt.title(f"Rolling Window Dynamic Quantile Regression VaR for {ticker}\nExceedances: {exceedances}")
        plt.xlabel("Date")
        plt.ylabel("Returns")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    return results, backtest_results


results_quantile, backtest_results = backtest_var(returns_df, out_of_sample_returns_df, alpha=0.01, lags=2, window=1000)


for ticker, results in backtest_results.items():
    print(f"\nBacktesting results for {ticker}:")
    print(f"Kupiec Test: {results['kupiec_result']}")
    print(f"Berkowitz Test: {results['berkowitz_result']}")
    print(f"Failure Rate: {results['failure_rate']:.2%}")
    print(f"Christoffersen Test: {results['christoffersen_result']}")
    print(f"Engle Manganelli Test: {results['engle_manganelli_result']}")

    tuff_result_garch  = results['tuff_result']
    print(f"TUFF Test: Time Until First Failure = {tuff_result_garch['time_to_failure']}, "
          f"Likelihood Ratio = {tuff_result_garch['likelihood_ratio']:.6f}, "
          f"P-value = {tuff_result_garch['p_value']:.6f}, Decision = {tuff_result_garch['decision']}")